{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lftfit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lftfit.py\n",
    "\n",
    "# FUNCTIONS\n",
    "    \n",
    "def fit_2point(t,x,fix='False',d=.3,C=8, return_best_fit_curve=False):\n",
    "    '''fit_2point(t,x,fix='False',d=.3,C=8):\n",
    "        Takes a pair of ALT/AST points and fits to the simple first-order kinetics model\n",
    "        t=list or np array of time values\n",
    "        x=list or np array of lab values (AST or ALT)\n",
    "        fix= whether to fix some of the parameters. Default = 'False' (with quotes!!), fit both params\n",
    "        ! need at least 3 points to fit both params\n",
    "        -- other options: 'd' (fix d; make sure to set d=value); 'C' (set 'C')\n",
    "        \n",
    "        returns result (gmodel output)\n",
    "        - access xfit by result.best_fit\n",
    "        - access result.params as result.params[key].value, where keys in this model are:\n",
    "        -- L0, C, d, derived_baseline_LFTs'''\n",
    "    \n",
    "    import lftmodels\n",
    "    from lmfit import Model\n",
    "    import numpy as np\n",
    "    \n",
    "    # t-correction\n",
    "    initial_t=t[0]\n",
    "    t_model=t-initial_t\n",
    "    \n",
    "    # create the lmfit model\n",
    "    gmodel = Model(lftmodels.ALT_no_heps)\n",
    "    \n",
    "    #ALT_no_heps(t, L0, C, a):\n",
    "    gmodel.set_param_hint('L0', value=x[0], vary=False)\n",
    "    gmodel.set_param_hint('derived_baseline_LFTs', expr='C/d')\n",
    "    \n",
    "    if fix == 'd':\n",
    "        gmodel.set_param_hint('C', value=8, min=0, vary=True)\n",
    "        gmodel.set_param_hint('d', value=d, min=0, vary=False)\n",
    "    elif fix == 'C':\n",
    "        gmodel.set_param_hint('C', value=C, min=0, vary=False)\n",
    "        gmodel.set_param_hint('d', value=.3, min=0, vary=True)\n",
    "    elif fix == 'False':\n",
    "        gmodel.set_param_hint('C', value=8, min=0, vary=True)\n",
    "        gmodel.set_param_hint('d', value=.3, min=0, vary=True)\n",
    "    params = gmodel.make_params()\n",
    "    \n",
    "    # fit the model\n",
    "    result = gmodel.fit(x, params, t=t_model)\n",
    "    \n",
    "    if return_best_fit_curve:\n",
    "        # add add'l datapoints\n",
    "        t_highres=np.linspace(0,np.ceil(t_model[-1]),50)\n",
    "        x_highres=gmodel.eval(result.params,t=t_highres)\n",
    "        \n",
    "        return result, t_highres+initial_t, x_highres\n",
    "    \n",
    "    return result\n",
    "\n",
    "def delta_EXP(t,x,d=.44, baseline_LFT=18, calc_UL_LL=True, d_UL=.8, d_LL=.35):\n",
    "    '''delta_EXP(t,x,fix='False',d=.3,C=8):\n",
    "        Takes a pair of ALT/AST points and calculates the difference between x2 and the expected x2* if following exponential decay according to parameters d and C\n",
    "        t=pair of time values\n",
    "        x=pair of AST or ALT values\n",
    "        \n",
    "        returns delta (which can be interpreted as *excess* AST or ALT present over what one expects given exponential decay with parameters d and C)'''\n",
    "    \n",
    "    import lftmodels\n",
    "    from lmfit import Model\n",
    "    import numpy as np\n",
    "    import scipy\n",
    "    \n",
    "    if len(t) != 2 or len(x) !=2:\n",
    "        print('delta_EXP expects only a pair of t1,t2 and x1,x2 values')\n",
    "        return None\n",
    "    \n",
    "    # calculate C from baseline\n",
    "    C = d*baseline_LFT\n",
    "    \n",
    "    # t-correction (in case we're passed mid-trajectory or non-zeroed time)\n",
    "    initial_t=t[0]\n",
    "    t_model=t-initial_t\n",
    "    \n",
    "    # ALT_no_heps(t, L0, C, d). This is the magic. Plug in t1,t2 (in t_model, zeroed), L0=X[0]. \n",
    "    EX_x = lftmodels.ALT_no_heps(t_model,x[0],C,d)\n",
    "    # now the model x[1] is the expected x[1] at time t[1] if exponential decay alone. \n",
    "    # ..therefore, return x[1] (actual, observed) minus EX_x[1] (expected)\n",
    "    delta_EX = x[1]-EX_x[1]\n",
    "    \n",
    "    if calc_UL_LL:\n",
    "        # repeat using UL and LL of d\n",
    "        ## UL ##\n",
    "        C_UL = d_UL*baseline_LFT\n",
    "        EX_x_UL = lftmodels.ALT_no_heps(t_model,x[0],C_UL,d_UL)\n",
    "        delta_UL = x[1]-EX_x_UL[1]\n",
    "        ## UL ##\n",
    "        C_LL = d_LL*baseline_LFT\n",
    "        EX_x_LL = lftmodels.ALT_no_heps(t_model,x[0],C_LL,d_LL)\n",
    "        delta_LL = x[1]-EX_x_LL[1]\n",
    "        \n",
    "        return delta_EX, delta_LL, delta_UL\n",
    "    else:\n",
    "        return delta_EX\n",
    "    \n",
    "def delta_BI(t,x,d1=.44, d2=1.76, baseline_LFT=18, calc_UL_LL=False, d_UL=.8, d_LL=.35):\n",
    "    '''delta_EXP(t,x,fix='False',d=.3,C=8):\n",
    "        Takes a pair of ALT/AST points and calculates the difference between x2 and the expected x2* if following exponential decay according to parameters d and C\n",
    "        t=pair of time values\n",
    "        x=pair of AST or ALT values\n",
    "        \n",
    "        returns delta (which can be interpreted as *excess* AST or ALT present over what one expects given exponential decay with parameters d and C)'''\n",
    "    \n",
    "    import lftmodels\n",
    "    from lmfit import Model\n",
    "    import numpy as np\n",
    "    import scipy\n",
    "    \n",
    "    if len(t) != 2 or len(x) !=2:\n",
    "        print('delta_EXP expects only a pair of t1,t2 and x1,x2 values')\n",
    "        return None\n",
    "    \n",
    "    # calculate C from baseline\n",
    "    C = d*baseline_LFT\n",
    "    \n",
    "    # t-correction (in case we're passed mid-trajectory or non-zeroed time)\n",
    "    initial_t=t[0]\n",
    "    t_model=t-initial_t\n",
    "    \n",
    "    # biexponential(t, C, L0, a, d1, d2). This is the magic. Plug in t1,t2 (in t_model, zeroed), L0=X[0]. \n",
    "    EX_x = lftmodels.biexponential(t_model,C, x[0],a=.6, d1, d2) #\n",
    "    # now the model x[1] is the expected x[1] at time t[1] if exponential decay alone. \n",
    "    # ..therefore, return x[1] (actual, observed) minus EX_x[1] (expected)\n",
    "    delta_EX = x[1]-EX_x[1]\n",
    "    \n",
    "#     if calc_UL_LL:\n",
    "#         # repeat using UL and LL of d\n",
    "#         ## UL ##\n",
    "#         C_UL = d_UL*baseline_LFT\n",
    "#         EX_x_UL = lftmodels.ALT_no_heps(t_model,x[0],C_UL,d_UL)\n",
    "#         delta_UL = x[1]-EX_x_UL[1]\n",
    "#         ## UL ##\n",
    "#         C_LL = d_LL*baseline_LFT\n",
    "#         EX_x_LL = lftmodels.ALT_no_heps(t_model,x[0],C_LL,d_LL)\n",
    "#         delta_LL = x[1]-EX_x_LL[1]\n",
    "        \n",
    "#         return delta_EX, delta_LL, delta_UL\n",
    "#     else:\n",
    "        return delta_EX\n",
    "\n",
    "def fit_biexp(t,x):\n",
    "    \n",
    "    import lftmodels\n",
    "    from lmfit import Model\n",
    "    import numpy as np\n",
    "    \n",
    "    # t-correction\n",
    "    initial_t=t[0]\n",
    "    t_model=t-initial_t\n",
    "    \n",
    "    # create the lmfit model\n",
    "    gmodel = Model(lftmodels.biexponential)\n",
    "    \n",
    "    #biexponential(t, A, B, d1, d2):\n",
    "    gmodel.set_param_hint('L0', value=x[0], vary=False)\n",
    "    \n",
    "    gmodel.set_param_hint('C', value=22, min=0, max=100, vary=True)\n",
    "    gmodel.set_param_hint('a', value=.5, min=0, max=1, vary=True)\n",
    "    gmodel.set_param_hint('d1', value=1, min=0, vary=True)\n",
    "    gmodel.set_param_hint('d2', value=2, min=0, vary=True)\n",
    "    \n",
    "    params = gmodel.make_params()\n",
    "    \n",
    "    # fit the model\n",
    "    result = gmodel.fit(x, params, t=t_model)\n",
    "    \n",
    "    t_highres=np.linspace(0,np.ceil(t_model[-1]),50)\n",
    "    x_highres=gmodel.eval(result.params,t=t_highres)\n",
    "        \n",
    "    return result, t_highres+initial_t, x_highres\n",
    "\n",
    "def fit_simple(ids,\n",
    "               lfts,\n",
    "               lab='ALT',\n",
    "               dates=[],\n",
    "               pause_for_input=True,\n",
    "               plot_figs=True,\n",
    "               show_residuals=False,\n",
    "               show_heps=False,\n",
    "               dtau=0,\n",
    "               min_peak2trough=5,\n",
    "               min_peak_height=100,\n",
    "               prominence=1,\n",
    "               opt_loopfit_stats=False,\n",
    "               eval_C_d=False,\n",
    "               eval_C=False,\n",
    "               d=.42):\n",
    "    \n",
    "    # WARNINGS / ISSUES\n",
    "    # - only 2x flex in the L0 param in case we implement lookback\n",
    "    \n",
    "    from scipy.stats import shapiro\n",
    "    from scipy.stats import normaltest\n",
    "    from lmfit import Model\n",
    "    import matplotlib.pyplot as plt\n",
    "    import lftlib\n",
    "    import lftmodels\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if opt_loopfit_stats:\n",
    "        print('Loopfit stats will be reported, be sure to have 3 outputs - df, plots_save, loopfit_stats')\n",
    "    \n",
    "    first_fit=True\n",
    "    inp='t'\n",
    "\n",
    "    # overall loop fitting stats (e.g., number that pass various filters)\n",
    "    num_peaks=0\n",
    "    num_meeting_traj_criteria=0\n",
    "    num_fit=0\n",
    "    \n",
    "    \n",
    "    for i in range(0,len(ids)):\n",
    "        if inp =='n':\n",
    "            break\n",
    "            \n",
    "        xalt, tseries = lftlib.get_traj(lfts, ids[i], lab=lab)\n",
    "        \n",
    "        if list(dates):\n",
    "            # we can pass lists of dates for each MRN\n",
    "            peak_dates=dates[i]\n",
    "            # d of get_peaks() captures dates. Expects a list of dates; if we're only passing single value, make it a list...\n",
    "            if isinstance(peak_dates,float):\n",
    "                peak_dates=[peak_dates]\n",
    "            peaks, n_peaks, LL, UL = lftlib.get_peaks(xalt, tseries, d=peak_dates, opt_prominence=prominence, plot_peaks=False, min_height=min_peak_height, min_datapoints=min_peak2trough, allow_edge_peaks=True, options='peak_to_min')\n",
    "        else:\n",
    "            # no specific dates to grab, return all peaks meeting criteria\n",
    "            peaks, n_peaks, LL, UL = lftlib.get_peaks(xalt, tseries, opt_prominence=prominence, plot_peaks=False, min_height=min_peak_height, min_datapoints=min_peak2trough, allow_edge_peaks=True, options='peak_to_min')\n",
    "\n",
    "        # make sure there IS at least one peak\n",
    "        if n_peaks > 0:\n",
    "            if i%100==0: print(i)\n",
    "            # cycle through peaks\n",
    "            for j in range(0,len(LL)):\n",
    "                num_peaks=num_peaks+1\n",
    "                x=xalt[LL[j]:UL[j]]\n",
    "\n",
    "                # necessitate there are least min_peak2trough datapoints in this series and that the initial value is greater than min_peak_height\n",
    "                if len(x) > min_peak2trough and x[0] > min_peak_height:\n",
    "                    num_meeting_traj_criteria = num_meeting_traj_criteria+1\n",
    "                    # truncate t to this particular range\n",
    "                    t=tseries[LL[j]:UL[j]] # absolute time\n",
    "                \n",
    "                    # create the lmfit model\n",
    "                    gmodel = Model(lftmodels.ALT_no_heps)\n",
    "\n",
    "                    # auto-extract the parameters and give them initial values lft_trajz(t, C_l, C_h, a, k, z, d, baseline):\n",
    "\n",
    "                    #ALT_no_heps(t, L0, C, a):\n",
    "                    # NEW PARAMS\n",
    "                    gmodel.set_param_hint('L0', value=x[0], min=x[0]*.2, max=x[0]*10, vary=True)\n",
    "                    gmodel.set_param_hint('C', value=8, min=0, vary=True)\n",
    "                    gmodel.set_param_hint('d', value=.3, min=0, vary=True)\n",
    "                    gmodel.set_param_hint('derived_baseline_LFTs', expr='C/d')\n",
    "                \n",
    "                    params = gmodel.make_params()\n",
    "\n",
    "                    # have fit test\n",
    "                    has_fit=False\n",
    "                    has_excess_tail=False\n",
    "                    \n",
    "                    # record some values for saving\n",
    "                    t0_rec=t[0]\n",
    "                    t0_date=lftlib.epoch_days_to_date(t0_rec)\n",
    "                    tfinal_rec=t[-1]\n",
    "                    tlength_rec=t[-1]-t[0]\n",
    "                    x0_rec=x[0]\n",
    "                    xfinal_rec=x[-1]\n",
    "                    \n",
    "                    # set t0 as first time point of this sim; dtau allows an offset for backward prediction\n",
    "                    t=t-t[0]+dtau\n",
    "                    \n",
    "                    # BONUS TRACKS\n",
    "                    # THREE POINT TRACK\n",
    "                    # no assumptions, what is C and d throughout the time course\n",
    "                    if eval_C_d == True:\n",
    "                        d_track=np.array([])\n",
    "                        C_track3=np.array([])\n",
    "                        # sequentially fit the 3 point model on a rolling basis from 0:2, to n-3:n lab values\n",
    "                        # - record d, C at each\n",
    "                        for s in range(0,len(t)-2):\n",
    "                            p1=s\n",
    "                            pn=s+3\n",
    "                            three_point = fit_2point(t[p1:pn]-t[p1],x[p1:pn],fix='False')\n",
    "                            d_track=np.append(d_track,three_point.params['d'].value)\n",
    "                            C_track3=np.append(C_track3,three_point.params['C'].value)\n",
    "                        \n",
    "                    # TWO POINT TRACK\n",
    "                    # assume d=d, what is C (and/or C/d) throughout the time course\n",
    "                    if eval_C == True:\n",
    "                        C_track2=np.array([])\n",
    "                        # sequentially fit the 2 point model on a rolling basis from 0:1, to n-1:n lab values\n",
    "                        # - record C at each assuming input d=d\n",
    "                        for s in range(0,len(t)-1):\n",
    "                            p1=s\n",
    "                            pn=s+2\n",
    "                            two_point = fit_2point(t[p1:pn]-t[p1],x[p1:pn],fix='d', d=d)\n",
    "                            C_track2=np.append(C_track2,two_point.params['C'].value)\n",
    "                    try:\n",
    "                        result = gmodel.fit(x, params, t=t)\n",
    "                        has_fit=True\n",
    "                    except:\n",
    "                        # smart debugging, why didn't the model fit?\n",
    "                        print('Troubleshooting ids index=' + str(i) + ', peak number ' + str(j))\n",
    "                        # common problems:\n",
    "                        # 1. Very long tail values\n",
    "                        #  *solution, truncate. *Approach: find t1/2 (time for this LFT to decline by half) -- say 3 days, and multiply by some number (e.g., 10-15)\n",
    "                        #    then truncate to 30-45 days, which should be enough time to capture the beginning of the tail\n",
    "                        print('Checking whether there is an excessively long tail...')\n",
    "                        #    first x value less than x12\n",
    "                        x12=((x[0]-x[-1])/2)+x[-1]\n",
    "                        x12_idx = np.argmax(x<x12)\n",
    "\n",
    "                        # therefore, the transition in time where x12 occurs is between x12_idx and it's previous value. use linear interpolation to get the actual value\n",
    "                        xpair=x[x12_idx-1:x12_idx+1] # only 2 values, the first value where x is less than x12, and the value right before it\n",
    "                        tpair=t[x12_idx-1:x12_idx+1]\n",
    "                        # have to flip and reorder since np.interp accepts an *x* value to interp y at, while we have a y-value (x12) to interp a t (x) at\n",
    "                        t12=np.interp(x12,np.flip(xpair),np.flip(tpair)) # gets the linear interp for t at x12 between these two points\n",
    "\n",
    "                        # calculate truncation based on number of half_lives < X, where X is number of half-lives (in theory) for x0 to become normal\n",
    "                        normal_transaminases=15 # low/conservative number for ALT and AST\n",
    "                        t12_to_normal=np.log(normal_transaminases/x[0])/np.log(.5) #number of half lives for initial value (x[0]) \n",
    "                        num_days_to_truncate = (t12_to_normal*2)*t12 # num half-lives X length of half-life X 2 (for add'l buffer)\n",
    "                        # do the truncation with some checks\n",
    "                        t_old=t\n",
    "                        t=t_old[t_old<num_days_to_truncate] # this will return t if num_days_to_truncate exceeds the last element of t (all values of t<num_days will be True)\n",
    "                        len_t_old=len(t_old)\n",
    "                        len_t = len(t) # 0 if same length, otherwise len_diff is the number of datapoints truncated; make sure this doesn't truncate to less than minimum\n",
    "                        x_old = x\n",
    "                        x=x[:len_t]\n",
    "                        if len_t_old-len_t > 0 and len_t>min_peak2trough:\n",
    "                            # a truncation did happen:\n",
    "                            print('Long tail present; truncating by ' + str(t_old[-1]-t[-1]) + ' days and ' + str(len_t_old-len_t) + ' datapoints. Attempting fit again...')\n",
    "                            # truncate x as well\n",
    "                            has_excess_tail=True\n",
    "\n",
    "                            try:\n",
    "                                result = gmodel.fit(x, params, t=t)\n",
    "                                print('... Success! This is the truncation of tail: ')\n",
    "                                if plot_figs:\n",
    "                                    plt.plot(t, x, 'b.', label='final')\n",
    "                                    plt.plot(t_old, x_old, 'rx', label='original')\n",
    "                                    plt.show()                            \n",
    "                                has_fit=True\n",
    "                            except:\n",
    "                                print('Truncation of excessive tail did not resolve issue, restoring full time course, but here is what couldnt be fit:')\n",
    "                                plt.plot(t, x, 'b.', label='data')\n",
    "                                plt.title('Trajectory with truncated excessive tail')\n",
    "                                plt.show()\n",
    "                                t_short=t\n",
    "                                x_short=x\n",
    "                                t=t_old\n",
    "                                x=x_old\n",
    "\n",
    "                    if has_fit:\n",
    "                        num_fit=num_fit+1\n",
    "\n",
    "                        # add add'l datapoints\n",
    "                        t_highres=np.linspace(0,np.ceil(t[-1]),50)\n",
    "                        x_highres=gmodel.eval(result.params,t=t_highres)\n",
    "\n",
    "                        if plot_figs:\n",
    "                            print(result.fit_report())\n",
    "                            # plt.plot(x, result.init_fit, 'k--', label='initial fit')\n",
    "                            plt.plot(t, result.best_fit, 'rx', label='best fit')\n",
    "                            plt.plot(t, x, 'b.', label='data')\n",
    "                            plt.plot(t_highres,x_highres,'--',color='tab:gray')\n",
    "                            # really subtle: if dates is an np.array, convert to list first otherwise it will fail. otherwise referencing is fine in this code block\n",
    "                            if list(dates):\n",
    "                                t_date=dates[i]-t0_rec\n",
    "                                plt.plot(t_date, 5, 'v')\n",
    "                                plt.annotate(\"Point 1\", (t_date, 5))\n",
    "                                \n",
    "                            if eval_C_d:\n",
    "                                # plot the 3point running C values; print the d values \n",
    "                                plt.plot(t[1:-1],C_track3,'ko')\n",
    "                                print('d values 3-point-running:')\n",
    "                                print(d_track)\n",
    "                            if eval_C:\n",
    "                                # plot the 3point running C values; print the d values \n",
    "                                plt.plot(t[1:],C_track2/d,'bo')\n",
    "                            \n",
    "                            plt.legend(loc='best')\n",
    "                            plt.title(label=ids[i] + ' | ' + t0_date)\n",
    "                            plt.xlabel('days')\n",
    "                            plt.ylabel(lab+' U/L')\n",
    "                            axes = plt.gca()\n",
    "        #                     axes.set_xlim([xmin,xmax])\n",
    "                            axes.set_ylim([0,result.best_fit[0]*1.2])\n",
    "                            plt.show()\n",
    "\n",
    "                            # show heps\n",
    "                            if show_heps:\n",
    "                                x_heps=lftmodels.lft_traj_baseh(t=t_highres, H0=result.params['H0'].value, a=result.params['a'].value, bl_heps=result.params['bl_heps'].value)\n",
    "                                plt.plot(t_highres, x_heps)\n",
    "                                plt.show()\n",
    "\n",
    "                            # show residuals\n",
    "                            if show_residuals:\n",
    "                                plt.plot(t,result.residual)\n",
    "                                plt.show()\n",
    "                                \n",
    "                            plt.plot(t,np.log(x))\n",
    "                            plt.title('semilog')\n",
    "                            plt.show()\n",
    "\n",
    "#                         stat, p = shapiro(result.residual)\n",
    "#                         print('Statistics_Shapiro=%.3f, p=%.3f' % (stat, p))\n",
    "#                         if len(result.residual) >= 8:\n",
    "#                             stat, p = normaltest(result.residual)\n",
    "#                             print('Statistics_normal_test=%.3f, p=%.3f' % (stat, p))\n",
    "#                         acorr, p = lftlib.discrete_autocorr(result.residual,h=1)\n",
    "#                         print('Statistics_autocorr_R1=%.3f, Z=%.3f' % (acorr, p))\n",
    "                        \n",
    "\n",
    "                        qual_error=lftlib.qualitative_error(t, x, result.residual)\n",
    "                        MAPE=lftlib.MAPE(t, x, result.residual)\n",
    "                        nRMSD=lftlib.nRMSD(t, x, result.residual)\n",
    "                        if plot_figs:\n",
    "                            print('Marc_qual_error=%.3f' % (qual_error))\n",
    "                            print('MAPE=%.3f' % (MAPE) )\n",
    "                            print('nRMSD=%.3f' % (nRMSD) )\n",
    "                    \n",
    "                        # capture this model, automatically unpack what parameters and build save structure\n",
    "                        if first_fit:\n",
    "                            print('yes got to first fit...')\n",
    "                            # get parameter names\n",
    "                            save_dict={}\n",
    "                            plots_save={}\n",
    "                            first_fit=False\n",
    "                            for key in result.params:\n",
    "                                # initialize save_dict\n",
    "                                save_dict[key+'_'+lab]=[]\n",
    "                                save_dict[key+'_stderr_'+lab]=[]\n",
    "                            # now initialize the rest of the dict\n",
    "                            save_dict['id_'+lab]=[]\n",
    "                            save_dict['qerr_'+lab]=[]\n",
    "                            save_dict['peak_'+lab]=[]\n",
    "                            save_dict['t0_'+lab]=[]\n",
    "                            save_dict['tf_'+lab]=[]\n",
    "                            save_dict['tlen_'+lab]=[]\n",
    "                            save_dict['x0_'+lab]=[]\n",
    "                            save_dict['xf_'+lab]=[]\n",
    "                            idx_save=[]\n",
    "                        \n",
    "                        # record values to save for this fit\n",
    "                        for key in result.params:\n",
    "                                # record fitted and derived parameters\n",
    "                                save_dict[key+lab].append(result.params[key].value)\n",
    "                                save_dict[key+'stderr'+lab].append(result.params[key].stderr)\n",
    "                                \n",
    "                        # record other fit parameters\n",
    "                        save_dict['qerr_'+lab].append(qual_error)\n",
    "                        save_dict['id_'+lab].append(ids[i])\n",
    "                        save_dict['peak_'+lab].append(peaks[j]) # index of t where this peak starts\n",
    "                        save_dict['t0_'+lab].append(t0_rec)\n",
    "                        save_dict['tf_'+lab].append(tfinal_rec)\n",
    "                        save_dict['tlen_'+lab].append(tlength_rec)\n",
    "                        save_dict['x0_'+lab].append(x0_rec)\n",
    "                        save_dict['xf_'+lab].append(xfinal_rec)\n",
    "                        idx_save.append(ids[i]+'_'+str(peaks[j]))\n",
    "                        plots_save[idx_save[-1]]={'t': t,\n",
    "                                                  'x': x,\n",
    "                                                  'xfit': result.best_fit,\n",
    "                                                  't_highres': t_highres,\n",
    "                                                  'x_highres': x_highres}\n",
    "                        if eval_C:\n",
    "                            plots_save[idx_save[-1]]['C_track2'] = C_track2\n",
    "                            plots_save[idx_save[-1]]['d'] = d\n",
    "                        if eval_C_d:\n",
    "                            plots_save[idx_save[-1]]['C_track3'] = C_track3\n",
    "                            plots_save[idx_save[-1]]['d_track3'] = d_track\n",
    "\n",
    "                    else:\n",
    "                        print('EXCEPTION: could not fit to this plot: ids index=' + str(i) + ', peak number ' + str(j))\n",
    "                        plt.plot(t, x, 'b.', label='data')\n",
    "                        plt.show()\n",
    "\n",
    "                    if pause_for_input:\n",
    "                        inp=input('continue?')\n",
    "                        if inp=='n':\n",
    "                            break\n",
    "      \n",
    "    # ie, if we didn't have a fit..\n",
    "    if first_fit:\n",
    "        # if we're getting here and first_fit still true, there were no fits\n",
    "        if opt_loopfit_stats:\n",
    "            return None, None, None\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        df = pd.DataFrame(save_dict,index=idx_save)\n",
    "\n",
    "        loopfit_stats={'num_peaks': num_peaks,\n",
    "                       'num_meeting_traj_criteria': num_meeting_traj_criteria,\n",
    "                       'num_fit': num_fit}\n",
    "\n",
    "    # df has all the things you'd want to look at, psave has a few additional things (exact parameter fitting conditions / mins / maxes etc)\n",
    "    if opt_loopfit_stats:\n",
    "        return df, plots_save, loopfit_stats\n",
    "    else:\n",
    "        return df, plots_save\n",
    "\n",
    "def truncate_long_tail(t,x):\n",
    "    #  *Approach: find t1/2 (time for this LFT to decline by half) -- say 3 days, and multiply by some number (e.g., 10-15)\n",
    "    #    then truncate to 30-45 days, which should be enough time to capture the beginning of the tail\n",
    "    \n",
    "    import numpy as np\n",
    "    print('Checking whether there is an excessively long tail...')\n",
    "    #    first x value less than x12\n",
    "    x12=((x[0]-x[-1])/2)+x[-1]\n",
    "    x12_idx = np.argmax(x<x12)\n",
    "\n",
    "    # therefore, the transition in time where x12 occurs is between x12_idx and it's previous value. use linear interpolation to get the actual value\n",
    "    xpair=x[x12_idx-1:x12_idx+1] # only 2 values, the first value where x is less than x12, and the value right before it\n",
    "    tpair=t[x12_idx-1:x12_idx+1]\n",
    "    # have to flip and reorder since np.interp accepts an *x* value to interp y at, while we have a y-value (x12) to interp a t (x) at\n",
    "    t12=np.interp(x12,np.flip(xpair),np.flip(tpair)) # gets the linear interp for t at x12 between these two points\n",
    "    print(t12-t[0])\n",
    "\n",
    "    # calculate truncation based on number of half_lives < X, where X is number of half-lives (in theory) for x0 to become normal\n",
    "    normal_transaminases=12 # low/conservative number for ALT and AST\n",
    "    t12_to_normal=np.log(normal_transaminases/x[0])/np.log(.5) #number of half lives for initial value (x[0]) \n",
    "    print('t12_to_normal')\n",
    "    print(t12_to_normal)\n",
    "    num_days_to_truncate5x = (t12_to_normal*5)+t[0] # num half-lives X length of half-life X 5 (for add'l buffer)\n",
    "    num_days_to_truncate10x = (t12_to_normal*10)+t[0] # num half-lives X length of half-life X 10 (for add'l buffer)\n",
    "    # accept no more than 1 value between 5 and 10\n",
    "    print('5x: ' + str(num_days_to_truncate5x) + ' | 10x: ' + str(num_days_to_truncate10x))\n",
    "    print('t:')\n",
    "    print(t)\n",
    "    # do the truncation with some checks\n",
    "    t_old=t\n",
    "    t5=t_old[t_old<num_days_to_truncate5x] # this will return t if num_days_to_truncate exceeds the last element of t (all values of t<num_days will be True)\n",
    "    t10 = t_old[t_old<num_days_to_truncate10x]\n",
    "    # now, if num_days_to_truncate5x and num_days_to_truncate10x differ, ie 1+ values falls in the 5x-10x half lives range, take **1 more value** past t5\n",
    "    if len(t5) < len(t10):\n",
    "        t=t10[0:len(t5)+1]\n",
    "    else:\n",
    "        t=t5\n",
    "    len_t_old=len(t_old)\n",
    "    len_t = len(t) # 0 if same length, otherwise len_diff is the number of datapoints truncated; make sure this doesn't truncate to less than minimum\n",
    "    x_old = x\n",
    "    x=x[:len_t]\n",
    "    if len_t_old-len_t > 0:\n",
    "        # a truncation did happen:\n",
    "        print('Long tail present; truncating by ' + str(t_old[-1]-t[-1]) + ' days and ' + str(len_t_old-len_t) + ' datapoints. Attempting fit again...')\n",
    "        # truncate x as well\n",
    "        has_excess_tail=True\n",
    "        \n",
    "    return t, x\n",
    "\n",
    "def truncate_steady_state(t,x,frac_change_cutoff=.01):\n",
    "    import numpy as np\n",
    "    \n",
    "    window=x[0]-x[-1]\n",
    "    frac_change_vector = ((x[0:-1]-x[1:])/(t[1:]-t[0:-1]))/window\n",
    "    state_change = frac_change_vector<frac_change_cutoff\n",
    "    \n",
    "    # make sure at least one value changed, otherwise they're all above cutoff\n",
    "    if np.any(state_change):\n",
    "        # plus 1 because we took delta first and lost an element. So if cutoff happens between the 1st and 2nd elements (0-based), delta is True at 1, and we should keep elements 0, 1, which means cutoff=2 (non-inclusive final indexing)\n",
    "        first_element_w_less_than_frac_change = np.argmax(state_change)+1\n",
    "    \n",
    "        # does not include the first element with less than frac change, includes last element before that element\n",
    "        x_return = x[0:first_element_w_less_than_frac_change]\n",
    "        t_return = t[0:first_element_w_less_than_frac_change]\n",
    "\n",
    "#         if len(x) != len(x_return):\n",
    "#             print('Truncated at steady state by ' + str(len(x)-len(x_return)))\n",
    "#             print(frac_change_vector)\n",
    "    else:\n",
    "        t_return=t\n",
    "        x_return=x\n",
    "        \n",
    "    return t_return, x_return\n",
    "\n",
    "def first_gap_present(t,max_gap=5):\n",
    "    if t[1]-t[0] > max_gap:        \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def generate_p2t_trajectories(savepath,\n",
    "                                ids,\n",
    "                                df_lfts,\n",
    "                                lab='ALT',\n",
    "                                min_peak2trough=5,\n",
    "                                min_peak_height=150,\n",
    "                                prominence=1, \n",
    "                                screen_first_gap=True,\n",
    "                                screen_steady_state=True,\n",
    "                                screen_steady_state_cutoff=.01,\n",
    "                                screen_long_tail=True):\n",
    "    \n",
    "    # WARNINGS / ISSUES\n",
    "\n",
    "    import lftlib\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pprint\n",
    "    \n",
    "    ## run metadata structure\n",
    "    p2t_run_meta = {}\n",
    "    # initialize some\n",
    "    num_peaks=0\n",
    "    num_meeting_traj_criteria=0\n",
    "    \n",
    "    p2t_traj_meta={}\n",
    "    p2t_trajectories={}\n",
    "    full_trajectories={}\n",
    "    first_fit=False\n",
    "\n",
    "    # now initialize the rest of the dict\n",
    "    p2t_traj_meta['id_'+lab]=[]\n",
    "    p2t_traj_meta['t0_'+lab]=[]\n",
    "    p2t_traj_meta['t0_date_'+lab]=[]\n",
    "    p2t_traj_meta['index_xt_peak_'+lab]=[]\n",
    "    p2t_traj_meta['tf_'+lab]=[]\n",
    "    p2t_traj_meta['tf_date_'+lab]=[]\n",
    "    p2t_traj_meta['index_xt_trough_'+lab]=[]\n",
    "    p2t_traj_meta['tlen_'+lab]=[]\n",
    "    p2t_traj_meta['x0_'+lab]=[]\n",
    "    p2t_traj_meta['xf_'+lab]=[]\n",
    "    p2t_traj_meta['xdelta_'+lab]=[]\n",
    "    idx_save=[]\n",
    "    \n",
    "    # pre-filter for the proper lab (this should dramatically speed up get_traj)\n",
    "    lfts = df_lfts[df_lfts.test_desc == lab].copy()\n",
    "    \n",
    "    for i in range(0,len(ids)):\n",
    "        # progress bar\n",
    "        if i%100==0: print(i)\n",
    "        \n",
    "        # extract the trajectory for this lab value\n",
    "        xlab, tseries = lftlib.get_traj(lfts, ids[i], lab=lab)\n",
    "        \n",
    "        # accepts a trajectory, and extracts the peaks \n",
    "        peaks, n_peaks, LL, UL = lftlib.get_peaks(xlab, tseries, opt_prominence=prominence, plot_peaks=False, min_height=min_peak_height, min_datapoints=min_peak2trough, allow_edge_peaks=True, options='peak_to_min')\n",
    "\n",
    "        # make sure there IS at least one peak\n",
    "        if n_peaks > 0:\n",
    "            # save full trajectories (and the peak parsing results)\n",
    "            full_trajectories[ids[i]]={'tseries': tseries,\n",
    "                                       'xlab': xlab,\n",
    "                                       'peaks': peaks,\n",
    "                                       'n_peaks': n_peaks,\n",
    "                                       'LL': LL,\n",
    "                                       'UL': UL}\n",
    "            \n",
    "            # cycle through peaks\n",
    "            for j in range(0,len(LL)):\n",
    "                num_peaks=num_peaks+1\n",
    "                x=xlab[LL[j]:UL[j]]\n",
    "                t=tseries[LL[j]:UL[j]]\n",
    "                                \n",
    "                # assess whether there is a large gap between the first and second observation (if so, fitting will be thrown off, plus most of the data is early in a traj)\n",
    "                if screen_first_gap:\n",
    "                    no_first_gap = not first_gap_present(t)\n",
    "                else:\n",
    "                    no_first_gap = True\n",
    "                    \n",
    "                # although get_peaks requires monotonically declining values, they could still be declining very slowly. That adds datapoints that aren't really contributing to fit\n",
    "                if no_first_gap and screen_steady_state:\n",
    "                    # here, because t and x are truncated as needed, no need for true/false; if too many values removed, the len(x) > min_peak2trough will catch\n",
    "                    t, x = truncate_steady_state(t,x,frac_change_cutoff=screen_steady_state_cutoff)\n",
    "\n",
    "                # long tails are bad. e.g., falling trajectory days 0-5 from 1200->50, and then 10 years later the next value is 25, the peak-finder would include that value way later since it's less than 50. but we don't want it.\n",
    "                if no_first_gap and len(x) > min_peak2trough and screen_long_tail:\n",
    "                    t, x = truncate_long_tail(t,x)\n",
    "                \n",
    "                # necessitate there are least min_peak2trough datapoints in this series and that the initial value is greater than min_peak_height\n",
    "                if no_first_gap and len(x) > min_peak2trough and x[0] > min_peak_height:\n",
    "                    num_meeting_traj_criteria = num_meeting_traj_criteria+1\n",
    "                \n",
    "                    # record some values for saving\n",
    "                    t0_rec=t[0]\n",
    "                    t0_date=lftlib.epoch_days_to_date(t0_rec)\n",
    "                    tfinal_rec=t[-1]\n",
    "                    tfinal_date=lftlib.epoch_days_to_date(t0_rec)\n",
    "                    tlength_rec=t[-1]-t[0]\n",
    "                    x0_rec=x[0]\n",
    "                    xfinal_rec=x[-1]\n",
    "                    xdelta=x[0]-x[-1]\n",
    "                    \n",
    "                    # set t0 as first time point of this sim\n",
    "                    t=t-t[0]\n",
    "                    \n",
    "                    # record other fit parameters\n",
    "                    p2t_traj_meta['id_'+lab].append(ids[i])\n",
    "                    p2t_traj_meta['t0_'+lab].append(t0_rec)\n",
    "                    p2t_traj_meta['t0_date_'+lab].append(t0_date)\n",
    "                    p2t_traj_meta['index_xt_peak_'+lab].append(LL[j]) # index of t where this peak starts\n",
    "                    p2t_traj_meta['tf_'+lab].append(tfinal_rec)\n",
    "                    p2t_traj_meta['tf_date_'+lab].append(tfinal_date)\n",
    "                    p2t_traj_meta['index_xt_trough_'+lab].append(UL[j]) # index of t where this peak starts, not inclusive (meant to be used like tseries[LL[j]:UL[j]])\n",
    "                    p2t_traj_meta['tlen_'+lab].append(tlength_rec)\n",
    "                    p2t_traj_meta['x0_'+lab].append(x0_rec)\n",
    "                    p2t_traj_meta['xf_'+lab].append(xfinal_rec)\n",
    "                    p2t_traj_meta['xdelta_'+lab].append(xdelta)\n",
    "                    idx_save.append(ids[i]+'_'+str(peaks[j]))\n",
    "                    p2t_trajectories[idx_save[-1]]={'t': t,\n",
    "                                              'x': x,\n",
    "                                              't0': t0_rec,\n",
    "                                              't0_timestamp': str(t0_date),\n",
    "                                              'tf': tfinal_rec,\n",
    "                                              'tf_timestamp': str(tfinal_date),\n",
    "                                              'id': ids[i],\n",
    "                                              'peak_num': peaks[j]}\n",
    "\n",
    "    # make sure we found at least one fit by checking whether idx_save is empty\n",
    "    if idx_save:\n",
    "        # save paths\n",
    "        p2t_run_meta_path = savepath+lab+'_p2t_run_meta.txt'\n",
    "        p2t_traj_meta_df_path = savepath+lab+'_p2t_traj_meta_df.csv'\n",
    "        p2t_trajectories_path = savepath+lab+'_p2t_trajectories.npy'\n",
    "        p2t_full_traj_path = savepath+lab+'_full_trajectories.npy'\n",
    "        \n",
    "        # save run data\n",
    "        p2t_run_meta={'peak_finding_mode': 'peak_to_trough',\n",
    "                      'lab': lab,\n",
    "                      'num_input_ids': len(ids),\n",
    "                      'num_peaks': num_peaks,\n",
    "                      'num_traj_meeting_criteria': num_meeting_traj_criteria,\n",
    "                      'min_peak2trough': min_peak2trough,\n",
    "                      'min_peak_height': min_peak_height,\n",
    "                      'prominence': prominence,\n",
    "                      'screen_first_gap': screen_first_gap,\n",
    "                      'screen_steady_state': screen_steady_state,\n",
    "                      'screen_steady_state_cutoff': screen_steady_state_cutoff,\n",
    "                      'screen_long_tail': screen_long_tail}\n",
    "        # write the run data in dict form\n",
    "        file1 = open(p2t_run_meta_path,'w')\n",
    "        file1.write(pprint.pformat(p2t_run_meta, width=1))\n",
    "        file1.close()\n",
    "        \n",
    "        # write p2t_traj_metadata\n",
    "        p2t_traj_meta_df = pd.DataFrame(p2t_traj_meta,index=idx_save)\n",
    "        p2t_traj_meta_df.to_csv(p2t_traj_meta_df_path)\n",
    "        \n",
    "        # write full and individual trajectories as dicts in numpy\n",
    "        np.save(p2t_trajectories_path, p2t_trajectories) \n",
    "        np.save(p2t_full_traj_path, full_trajectories)\n",
    "        \n",
    "        return p2t_traj_meta_df, p2t_trajectories, full_trajectories, p2t_run_meta\n",
    "    \n",
    "    else:\n",
    "        print('No trajectories meeting criteria')\n",
    "        return None, None, None, None\n",
    "    \n",
    "    \n",
    "def fit_models(traj,\n",
    "               lab='ALT',\n",
    "               pause_for_input=True,\n",
    "               plot_figs=True,\n",
    "               dtau=0,\n",
    "               fit_simple_exponential=True,\n",
    "               fit_sq_impulse=True,\n",
    "               fit_biexponential=True,\n",
    "               fit_full_heps=False,\n",
    "               calculate_delta_EX=True,\n",
    "               eval_C_d=False,\n",
    "               eval_C=False,\n",
    "               d=.42,\n",
    "               d_LL=.22,\n",
    "               d_UL=.62,\n",
    "               specific_ids=False,\n",
    "               ids=[]):\n",
    "    \n",
    "    # WARNINGS / ISSUES\n",
    "    # - only 2x flex in the L0 param in case we implement lookback\n",
    "    \n",
    "    from scipy.stats import shapiro\n",
    "    from scipy.stats import normaltest\n",
    "    from lmfit import Model\n",
    "    import matplotlib.pyplot as plt\n",
    "    import lftlib\n",
    "    import lftmodels\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # copy traj for output\n",
    "    traj_out=traj.copy()\n",
    "    \n",
    "    # prime the save structure\n",
    "    save_dict={}\n",
    "    idx_save=[]\n",
    "#     models_meta = ['EX','BI','FH']\n",
    "    models_meta = []\n",
    "    if fit_simple_exponential:\n",
    "        models_meta.append('EX')\n",
    "    if fit_biexponential:\n",
    "        models_meta.append('BI')\n",
    "    if fit_full_heps:\n",
    "        models_meta.append('FH')\n",
    "    if fit_sq_impulse:\n",
    "        models_meta.append('SQ')\n",
    "    model_params = {}\n",
    "    model_params['EX'] = ['L0', 'C', 'd', 'drvBLFT']\n",
    "    model_params['SQ'] = ['L0', 'C', 'k', 'tau', 'd']\n",
    "    model_params['BI'] = ['L0', 'C', 'a', 'd1', 'd2']\n",
    "    model_params['FH'] = ['H0', 'L0', 'bLFT', 'bHEP', 'a', 'd', 'drvK', 'drvZ','drvFracInjur']\n",
    "    fit_meta = ['qerr', 'nRMSD', 'aic', 'bic', 'chisqr', 'redchi',]\n",
    "    \n",
    "    for model in models_meta:\n",
    "        for param in model_params[model]:\n",
    "            save_dict[param+'_'+model+'_'+lab]=[]\n",
    "            save_dict[param+'_stderr_'+model+'_'+lab]=[]\n",
    "        for fitstat in fit_meta:\n",
    "            # initialize save_dict (which will become a pandas df)\n",
    "            save_dict[fitstat+'_'+model+'_'+lab]=[]\n",
    "        \n",
    "    # these are custom error functions (ie not returned by lmfit) so we can't automatically iterate through them in a model.result, will add manually. But the save_dict is primed\n",
    "    fit_meta.remove('nRMSD')\n",
    "    fit_meta.remove('qerr')\n",
    "    \n",
    "    inp='t'\n",
    "    num_traj=0\n",
    "    \n",
    "    if specific_ids:\n",
    "        trajlist = ids\n",
    "    else:\n",
    "        trajlist = traj\n",
    "    \n",
    "    for idx in trajlist:\n",
    "        # keep track of total traj, vs fit traj\n",
    "        num_traj = num_traj+1\n",
    "        # progress bar\n",
    "        if num_traj%100==0: print(num_traj)\n",
    "        # save ids as list for pandas df\n",
    "        idx_save.append(idx)\n",
    "        \n",
    "        # extract x, t\n",
    "        t = traj[idx]['t']\n",
    "        x = traj[idx]['x']\n",
    "        \n",
    "        # calculate 'high res' t\n",
    "        t_highres = np.linspace(0,np.ceil(t[-1]),50)\n",
    "        # and save\n",
    "        traj_out[idx]['t_highres'] = t_highres\n",
    "        \n",
    "        # dtau allows an offset for backward prediction (defaults 0, generally leave it there)\n",
    "        t=t+dtau\n",
    "        \n",
    "        # models\n",
    "        EX_has_fit=False\n",
    "        \n",
    "        if fit_simple_exponential:\n",
    "                        \n",
    "            # SIMPLE EXPONENTIAL WITH CONSTANT C HEPATOCYTE DEATH\n",
    "            EX_model = Model(lftmodels.ALT_no_heps)\n",
    "\n",
    "            #ALT_no_heps(t, L0, C, a):\n",
    "            # NEW PARAMS\n",
    "            EX_model.set_param_hint('L0', value=x[0], min=x[0]*.2, max=x[0]*10, vary=True)\n",
    "            EX_model.set_param_hint('C', value=8, min=0, vary=True)\n",
    "            EX_model.set_param_hint('d', value=.4, min=0, vary=True)\n",
    "            EX_model.set_param_hint('drvBLFT', expr='C/d')\n",
    "\n",
    "            EX_params = EX_model.make_params()\n",
    "\n",
    "            try:\n",
    "                EX_result = EX_model.fit(x, EX_params, t=t)\n",
    "                EX_has_fit=True\n",
    "\n",
    "            except:\n",
    "                print('failed EXP fit:')\n",
    "                print(idx)\n",
    "                # store nan values in all elements\n",
    "                for param in model_params['EX']:\n",
    "                    save_dict[param+'_EX_'+lab].append(np.nan)\n",
    "                    save_dict[param+'_stderr_EX_'+lab].append(np.nan)\n",
    "                # custom errors\n",
    "                save_dict['qerr_EX_'+lab].append(np.nan)\n",
    "                save_dict['nRMSD_EX_'+lab].append(np.nan)\n",
    "                # iterate through remainder of result.[error]\n",
    "                for fitstat in fit_meta:\n",
    "                    save_dict[fitstat+'_EX_'+lab].append(np.nan)\n",
    "\n",
    "            if EX_has_fit:\n",
    "                #highres solution\n",
    "                x_highres_EX=EX_model.eval(EX_result.params,t=t_highres)\n",
    "                # save to traj\n",
    "                traj_out[idx]['x_EX'] = EX_result.best_fit\n",
    "                traj_out[idx]['x_highres_EX'] = x_highres_EX\n",
    "\n",
    "                EX_qual_error=lftlib.qualitative_error(t, x, EX_result.residual)\n",
    "                EX_nRMSD=lftlib.nRMSD(t, x, EX_result.residual)\n",
    "\n",
    "                # save fit results\n",
    "                for param in model_params['EX']:\n",
    "                    save_dict[param+'_EX_'+lab].append(EX_result.params[param].value)\n",
    "                    save_dict[param+'_stderr_EX_'+lab].append(EX_result.params[param].stderr)\n",
    "                # custom errors\n",
    "                save_dict['qerr_EX_'+lab].append(EX_qual_error)\n",
    "                save_dict['nRMSD_EX_'+lab].append(EX_nRMSD)\n",
    "                # iterate through remainder of result.[error]\n",
    "                for fitstat in fit_meta:\n",
    "                    save_dict[fitstat+'_EX_'+lab].append(getattr(EX_result,fitstat))\n",
    "        \n",
    "        # models\n",
    "        SQ_has_fit=False\n",
    "        \n",
    "        if fit_sq_impulse:\n",
    "                        \n",
    "            # SIMPLE SQPONENTIAL WITH CONSTANT C HEPATOCYTE DEATH\n",
    "            SQ_model = Model(lftmodels.square_impulse)\n",
    "\n",
    "            #ALT_no_heps(t, L0, C, a):\n",
    "            # NEW PARAMS\n",
    "            SQ_model.set_param_hint('L0', value=x[0], min=x[0]*.2, max=x[0]*10, vary=True)\n",
    "            SQ_model.set_param_hint('C', value=8, min=0, vary=True)\n",
    "            SQ_model.set_param_hint('d', value=.44, min=0, vary=True)\n",
    "            SQ_model.set_param_hint('tau', value=1, min=0, max=t[-1], vary=True)\n",
    "            SQ_model.set_param_hint('k', value=100, min=0, vary=True)\n",
    "\n",
    "            SQ_params = SQ_model.make_params()\n",
    "\n",
    "            try:\n",
    "                SQ_result = SQ_model.fit(x, SQ_params, t=t)\n",
    "                SQ_has_fit=True\n",
    "\n",
    "            except:\n",
    "                print('failed SQ fit with tau=1, changing tau to midpoint..')\n",
    "                SQ_params['tau'].value=(t[-1]-t[0])/2\n",
    "                try:\n",
    "                    SQ_result = SQ_model.fit(x, SQ_params, t=t)\n",
    "                    SQ_has_fit=True\n",
    "                except:\n",
    "                    print('failed SQ fit with tau=midpoint')\n",
    "                    print(idx)\n",
    "                    # store nan values in all elements\n",
    "                    for param in model_params['SQ']:\n",
    "                        save_dict[param+'_SQ_'+lab].append(np.nan)\n",
    "                        save_dict[param+'_stderr_SQ_'+lab].append(np.nan)\n",
    "                    # custom errors\n",
    "                    save_dict['qerr_SQ_'+lab].append(np.nan)\n",
    "                    save_dict['nRMSD_SQ_'+lab].append(np.nan)\n",
    "                    # iterate through remainder of result.[error]\n",
    "                    for fitstat in fit_meta:\n",
    "                        save_dict[fitstat+'_SQ_'+lab].append(np.nan)\n",
    "\n",
    "            if SQ_has_fit:\n",
    "                #highres solution\n",
    "                x_highres_SQ=SQ_model.eval(SQ_result.params,t=t_highres)\n",
    "                # save to traj\n",
    "                traj_out[idx]['x_SQ'] = SQ_result.best_fit\n",
    "                traj_out[idx]['x_highres_SQ'] = x_highres_SQ\n",
    "\n",
    "                SQ_qual_error=lftlib.qualitative_error(t, x, SQ_result.residual)\n",
    "                SQ_nRMSD=lftlib.nRMSD(t, x, SQ_result.residual)\n",
    "\n",
    "                # save fit results\n",
    "                for param in model_params['SQ']:\n",
    "                    save_dict[param+'_SQ_'+lab].append(SQ_result.params[param].value)\n",
    "                    save_dict[param+'_stderr_SQ_'+lab].append(SQ_result.params[param].stderr)\n",
    "                # custom errors\n",
    "                save_dict['qerr_SQ_'+lab].append(SQ_qual_error)\n",
    "                save_dict['nRMSD_SQ_'+lab].append(SQ_nRMSD)\n",
    "                # iterate through remainder of result.[error]\n",
    "                for fitstat in fit_meta:\n",
    "                    save_dict[fitstat+'_SQ_'+lab].append(getattr(SQ_result,fitstat))\n",
    "        \n",
    "        \n",
    "        BI_has_fit=False\n",
    "                \n",
    "        if fit_biexponential:\n",
    "\n",
    "            # create the lmfit model\n",
    "            BI_model = Model(lftmodels.biexponential)\n",
    "            \n",
    "            # NEW PARAMS\n",
    "            BI_model.set_param_hint('L0', value=x[0], min=x[0]*.2, max=x[0]*10, vary=True)\n",
    "            BI_model.set_param_hint('C', value=8, min=0, vary=True)\n",
    "            BI_model.set_param_hint('a', value=.5, min=0, max=1, vary=True)\n",
    "            BI_model.set_param_hint('d1', value=.5, min=0, vary=True)\n",
    "            BI_model.set_param_hint('d2', value=2, min=0, vary=True)\n",
    "\n",
    "            BI_params = BI_model.make_params()\n",
    "            \n",
    "            try:\n",
    "                BI_result = BI_model.fit(x, BI_params, t=t)\n",
    "                BI_has_fit=True\n",
    "                \n",
    "            except:\n",
    "                print('failed BI fit:')\n",
    "                print(idx)\n",
    "                # store nan values in all elements\n",
    "                for param in model_params['BI']:\n",
    "                    save_dict[param+'_BI_'+lab].append(np.nan)\n",
    "                    save_dict[param+'_stderr_BI_'+lab].append(np.nan)\n",
    "                # custom errors\n",
    "                save_dict['qerr_BI_'+lab].append(np.nan)\n",
    "                save_dict['nRMSD_BI_'+lab].append(np.nan)\n",
    "                # iterate through remainder of result.[error]\n",
    "                for fitstat in fit_meta:\n",
    "                    save_dict[fitstat+'_BI_'+lab].append(np.nan)\n",
    "            \n",
    "            if BI_has_fit:\n",
    "                #highres solution\n",
    "                x_highres_BI=BI_model.eval(BI_result.params,t=t_highres)\n",
    "                # save to traj\n",
    "                traj_out[idx]['x_BI'] = BI_result.best_fit\n",
    "                traj_out[idx]['x_highres_BI'] = x_highres_BI\n",
    "                \n",
    "                BI_qual_error=lftlib.qualitative_error(t, x, BI_result.residual)\n",
    "                BI_nRMSD=lftlib.nRMSD(t, x, BI_result.residual)\n",
    "                \n",
    "                # save results\n",
    "                for param in model_params['BI']:\n",
    "                    save_dict[param+'_BI_'+lab].append(BI_result.params[param].value)\n",
    "                    save_dict[param+'_stderr_BI_'+lab].append(BI_result.params[param].stderr)\n",
    "                # custom errors\n",
    "                save_dict['qerr_BI_'+lab].append(BI_qual_error)\n",
    "                save_dict['nRMSD_BI_'+lab].append(BI_nRMSD)\n",
    "                # iterate through remainder of result.[error]\n",
    "                for fitstat in fit_meta:\n",
    "                    save_dict[fitstat+'_BI_'+lab].append(getattr(BI_result,fitstat))\n",
    "        \n",
    "                \n",
    "        FH_has_fit=False\n",
    "                \n",
    "        if fit_full_heps:\n",
    "            \n",
    "            # heps factor\n",
    "            hfactor=1/1000000\n",
    "\n",
    "            # baseline estimate\n",
    "            baseline_ALT = x[-1]\n",
    "            baseline_heps = 927*107*(10**6)*hfactor\n",
    "\n",
    "            init_d=0.3\n",
    "            init_a=10**8*hfactor\n",
    "\n",
    "            # create the lmfit model\n",
    "            FH_model = Model(lftmodels.lft_traj_base)\n",
    "            \n",
    "            # NEW PARAMS\n",
    "            FH_model.set_param_hint('H0', value=baseline_heps*.95, min=baseline_heps*.5, max=baseline_heps, vary=True)\n",
    "            FH_model.set_param_hint('L0', value=x[0], min=x[0]*.5, max=x[0]*2, vary=True)\n",
    "            FH_model.set_param_hint('bLFT', value=baseline_ALT, min=10, max=2*x[-1], vary=True)\n",
    "            FH_model.set_param_hint('bHEP', value=baseline_heps, min=.75*baseline_heps, vary=False)\n",
    "            FH_model.set_param_hint('a', value=init_a, min=init_a*.01, max=baseline_heps/30, vary=True)\n",
    "            FH_model.set_param_hint('d', value=.3, min=0, vary=True)\n",
    "            FH_model.set_param_hint('drvK', expr='a/bHEP')\n",
    "            FH_model.set_param_hint('drvZ', expr='bLFT*d/a')\n",
    "            FH_model.set_param_hint('drvFracInjur', expr='1-H0/bHEP')\n",
    "\n",
    "            FH_params = FH_model.make_params()\n",
    "            \n",
    "            try:\n",
    "                FH_result = FH_model.fit(x, FH_params, t=t)\n",
    "                FH_has_fit=True\n",
    "                \n",
    "            except:\n",
    "                print('failed FH fit, allowing baseline heps to vary...') \n",
    "                FH_params['bHEP'].vary=True\n",
    "                try: \n",
    "                    FH_result = FH_model.fit(x, FH_params, t=t)\n",
    "                    FH_has_fit=True\n",
    "                except:\n",
    "                    print('FH fit failed:')\n",
    "                    print(idx)\n",
    "                \n",
    "                    # store nan values in all elements\n",
    "                    for param in model_params['FH']:\n",
    "                        save_dict[param+'_FH_'+lab].append(np.nan)\n",
    "                        save_dict[param+'_stderr_FH_'+lab].append(np.nan)\n",
    "                    # custom errors\n",
    "                    save_dict['qerr_FH_'+lab].append(np.nan)\n",
    "                    save_dict['nRMSD_FH_'+lab].append(np.nan)\n",
    "                    # iterate through remainder of result.[error]\n",
    "                    for fitstat in fit_meta:\n",
    "                        save_dict[fitstat+'_FH_'+lab].append(np.nan)\n",
    "            \n",
    "            if FH_has_fit:\n",
    "                #highres solution\n",
    "                x_highres_FH=FH_model.eval(FH_result.params,t=t_highres)\n",
    "                # save to traj\n",
    "                traj_out[idx]['x_FH'] = FH_result.best_fit\n",
    "                traj_out[idx]['x_highres_FH'] = x_highres_FH\n",
    "                \n",
    "                FH_qual_error=lftlib.qualitative_error(t, x, FH_result.residual)\n",
    "                FH_nRMSD=lftlib.nRMSD(t, x, FH_result.residual)\n",
    "                \n",
    "                # save results\n",
    "                for param in model_params['FH']:\n",
    "                    save_dict[param+'_FH_'+lab].append(FH_result.params[param].value)\n",
    "                    save_dict[param+'_stderr_FH_'+lab].append(FH_result.params[param].stderr)\n",
    "                # custom errors\n",
    "                save_dict['qerr_FH_'+lab].append(FH_qual_error)\n",
    "                save_dict['nRMSD_FH_'+lab].append(FH_nRMSD)\n",
    "                # iterate through remainder of result.[error]\n",
    "                for fitstat in fit_meta:\n",
    "                    save_dict[fitstat+'_FH_'+lab].append(getattr(FH_result,fitstat))\n",
    "                \n",
    "        if calculate_delta_EX:\n",
    "            delta_EX_track=np.array([])\n",
    "            delta_EX_LL_track=np.array([])\n",
    "            delta_EX_UL_track=np.array([])\n",
    "            # sequentially fit the delta_EX score on a rolling basis from 0:1, to n-1:n lab values\n",
    "            # - record delta at each assuming input d=d\n",
    "            for s in range(0,len(t)-1):\n",
    "                p1=s\n",
    "                pn=s+2\n",
    "                delta_EX_i, LL, UL = delta_EXP(t[p1:pn],x[p1:pn],d=d, baseline_LFT=0, calc_UL_LL=True, d_UL=d_UL, d_LL=d_LL)\n",
    "                delta_EX_track=np.append(delta_EX_track,delta_EX_i)\n",
    "                delta_EX_LL_track=np.append(delta_EX_LL_track,LL)\n",
    "                delta_EX_UL_track=np.append(delta_EX_UL_track,UL)\n",
    "                \n",
    "            traj_out[idx]['delta_EX'] = delta_EX_track\n",
    "            traj_out[idx]['delta_EX_LL'] = delta_EX_LL_track\n",
    "            traj_out[idx]['delta_EX_UL'] = delta_EX_UL_track\n",
    "        \n",
    "        # THREE POINT TRACK\n",
    "        # no assumptions, what is C and d throughout the time course\n",
    "        if eval_C_d:\n",
    "            d_track=np.array([])\n",
    "            C_track3=np.array([])\n",
    "            # sequentially fit the 3 point model on a rolling basis from 0:2, to n-3:n lab values\n",
    "            # - record d, C at each\n",
    "            for s in range(0,len(t)-2):\n",
    "                p1=s\n",
    "                pn=s+3\n",
    "                three_point = fit_2point(t[p1:pn]-t[p1],x[p1:pn],fix='False')\n",
    "                d_track=np.append(d_track,three_point.params['d'].value)\n",
    "                C_track3=np.append(C_track3,three_point.params['C'].value)\n",
    "\n",
    "        # TWO POINT TRACK\n",
    "        # assume d=d, what is C (and/or C/d) throughout the time course\n",
    "        if eval_C:\n",
    "            C_track2=np.array([])\n",
    "            # sequentially fit the 2 point model on a rolling basis from 0:1, to n-1:n lab values\n",
    "            # - record C at each assuming input d=d\n",
    "            for s in range(0,len(t)-1):\n",
    "                p1=s\n",
    "                pn=s+2\n",
    "                two_point = fit_2point(t[p1:pn]-t[p1],x[p1:pn],fix='d', d=d)\n",
    "                C_track2=np.append(C_track2,two_point.params['C'].value)\n",
    "\n",
    "        if plot_figs:\n",
    "            print(result.fit_report())\n",
    "            # plt.plot(x, result.init_fit, 'k--', label='initial fit')\n",
    "            plt.plot(t, result.best_fit, 'rx', label='best fit')\n",
    "            plt.plot(t, x, 'b.', label='data')\n",
    "            plt.plot(t_highres,x_highres,'--',color='tab:gray')\n",
    "            # really subtle: if dates is an np.array, convert to list first otherwise it will fail. otherwise referencing is fine in this code block\n",
    "            if list(dates):\n",
    "                t_date=dates[i]-t0_rec\n",
    "                plt.plot(t_date, 5, 'v')\n",
    "                plt.annotate(\"Point 1\", (t_date, 5))\n",
    "\n",
    "            if eval_C_d:\n",
    "                # plot the 3point running C values; print the d values \n",
    "                plt.plot(t[1:-1],C_track3,'ko')\n",
    "                print('d values 3-point-running:')\n",
    "                print(d_track)\n",
    "            if eval_C:\n",
    "                # plot the 3point running C values; print the d values \n",
    "                plt.plot(t[1:],C_track2/d,'bo')\n",
    "                \n",
    "    df = pd.DataFrame(save_dict,index=idx_save)\n",
    "    \n",
    "    return df, traj_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
