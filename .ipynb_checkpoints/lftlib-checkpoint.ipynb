{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lftlib.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lftlib.py\n",
    "\n",
    "def load_RPDR_labs_multiple(dir_data, path_labs, path_synonyms, datetime_col='Seq_Date_Time', result_col='Result', test_col='Group_Id', delim='|', clean_columns=True):\n",
    "    ''' load_labs_multiple(dir_data, path_labs, path_synonyms, datetime_col='Seq_Date_Time', result_col='Result', test_col='Group_Id', delim='|', clean_columns=True):\n",
    "        Sequentially loads all files from RPDR data dump when multiple files have the same name. \n",
    "        \n",
    "        1. Starts in dir_data (should have trailing slash), grabs all sub-folders' names automatically, then sequentially loads: dir_data/[sub-folders]/path_labs (where path_labs is the name of the file)\n",
    "        * Note for whatever reason, on a multiple-split file dump from RPDR the labs, demographics, etc files are all named the exact same, just in different zips\n",
    "        2. Calls the traditional load LFTs function on each file\n",
    "        3. Concatenates all results and returns 1 DF (may be VERY large. ~3-4 gb / 80,000 patients)\n",
    "        \n",
    "        See load_native_data for remainder of parameters which are passed to that function\n",
    "        \n",
    "        '''\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # get list of subdirectories\n",
    "    subdirectories = [x[0] for x in os.walk(dir_data)][1:]\n",
    "    \n",
    "    first=True\n",
    "    # for each subdir, use the traditional load function to load data and concat\n",
    "    for subdir in subdirectories:\n",
    "        path_to_labs=subdir+'/'+path_labs\n",
    "        lfts, discarded = load_RPDR_labs(path=path_to_labs,\n",
    "                                           path_synonyms=path_synonyms,\n",
    "                                           datetime_col=datetime_col,\n",
    "                                           result_col=result_col,\n",
    "                                           test_col=test_col, \n",
    "                                           delim=delim,\n",
    "                                           clean_columns=clean_columns)\n",
    "        \n",
    "        if first==True:\n",
    "            concat_pd = lfts\n",
    "            first=False\n",
    "        else:\n",
    "            concat_pd=pd.concat([concat_pd, lfts],ignore_index=True)\n",
    "    \n",
    "    return concat_pd\n",
    "\n",
    "def load_RPDR_labs(path,path_synonyms,datetime_col='Seq_Date_Time', result_col='Result', test_col='Group_Id', delim='|', clean_columns=True):\n",
    "    '''load_RPDR_labs(path,path_synonyms,datetime_col='Seq_Date_Time', result_col='Result', test_col='Group_Id', delim='|', clean_columns=True):\n",
    "    \n",
    "    DESC: This is the main labs loading function, for loading labs data from an RPDR data pull, and processing lab results. \n",
    "     Does MANY THINGS. \n",
    "     1. Homogenizes column names:\n",
    "         rename result_col to 'result' if not already named result\n",
    "         rename datetime_col to 'datetime' if not already named so; convert this to pd.DateTime format\n",
    "         rename test_col to 'test_desc'\n",
    "    path: path to labs file (in .csv or other text delimited file)\n",
    "    path_synonyms: path to synonyms file (.csv). structure of synonyms file: first row is what you want to call each lab \n",
    "     defined in the test_col column; remainder of rows identify the exact text of labs that should be considered equivalent. \n",
    "     All of these will be homogenized to the row 1 name\n",
    "    datetime_col: name of column with date/time of lab test\n",
    "    result_col: name of column that contains the result of the lab test\n",
    "    test_col: name of the column that contains the name of the lab test (this is what synonyms acts upon)\n",
    "    delim: delimiter of the labs file (the file pointed to by path)\n",
    "    clean_columns: bool. If true, removes all columns except those named the following:\n",
    "     [['EMPI', 'test_desc', 'datetime', 'result_orig', 'result', 'MRN_Type', 'MRN', 'Test_Description', 'Result_Text', 'above_below_assay_limit']]\n",
    "    \n",
    "    RETURNS: lfts, discarded_rows\n",
    "     lfts is the labs as panda database (should really be named labs, but initially developed for only liver function tests (LFTs))\n",
    "     discarded_rows is a second pandas database containing anything removed by the processing\n",
    "     \n",
    "    Note: input result column will be returned as 'result_orig' and the processed result column will be returned as 'result'\n",
    "    '''\n",
    "    # read data\n",
    "    # \n",
    "    # WARNINGS:\n",
    "    # - default behavior is to remove < and >; could add a new column to mark these events if pertinent later\n",
    "    import pandas as pd\n",
    "    \n",
    "    # read the data\n",
    "    print('Loading data from ' + path)\n",
    "    lfts = pd.read_csv(path,delimiter=delim)\n",
    "    \n",
    "#     if result_col != 'result':\n",
    "#         lfts['result'] = lfts.loc[:,result_col]\n",
    "#     else:\n",
    "#         # result col is already named result. rename it\n",
    "#         lfts['result_orig'] = lfts['result']\n",
    "\n",
    "    ## Mod 1 ##\n",
    "    if result_col in lfts.columns:\n",
    "        lfts.rename(columns={result_col: 'result_orig'}, inplace=True)\n",
    "        lfts['result'] = lfts.loc[:,'result_orig']\n",
    "    else:\n",
    "        raise ValueError('Incorrect name specified for result column')\n",
    "    ## Mod 1 ##\n",
    "    \n",
    "    # convert datetime column to datetime format\n",
    "    if datetime_col != 'datetime':\n",
    "        lfts['datetime'] = pd.to_datetime(lfts.loc[:,datetime_col])\n",
    "        lfts.drop(labels=datetime_col, axis=1, inplace=True)\n",
    "    else:\n",
    "        lfts['datetime'] = pd.to_datetime(lfts['datetime'])\n",
    "    \n",
    "    if test_col != 'test_desc':\n",
    "        lfts['test_desc'] = lfts.loc[:,test_col]\n",
    "    else:\n",
    "        # test_desc col is already named test_desc. rename it\n",
    "        lfts['test_desc_orig'] = lfts['test_desc']\n",
    "    \n",
    "    ######################\n",
    "    # Identify synonyms of labs and homogenize based on a file called lab_synonyms.csv\n",
    "    # structure: first row is what you want to call each lab; below the col are all the synonyms to change to the header name\n",
    "    syn=pd.read_csv(path_synonyms)\n",
    "    correct_lab_names=syn.columns.tolist()\n",
    "    for correct_lab in correct_lab_names:\n",
    "        alternate_names_to_replace=syn[correct_lab].tolist()\n",
    "        for alt in alternate_names_to_replace:\n",
    "            # irregular length columns, if this column has nan's at the end, don't cycle through them\n",
    "            if isinstance(alt,str):\n",
    "                lfts.test_desc.replace(alt,correct_lab,regex=False, inplace=True)\n",
    "            \n",
    "    # error checking\n",
    "    if set(correct_lab_names) == set(lfts.test_desc.unique().tolist()):\n",
    "        print('Successful homogenization of lab names')\n",
    "        print(correct_lab_names)\n",
    "    else:\n",
    "        got_list=set(lfts.test_desc.unique().tolist())\n",
    "        expected_list=set(correct_lab_names)\n",
    "        print('FAILED TO HOMOGENIZE NAMES')\n",
    "        print('Expected : ')\n",
    "        print(expected_list)\n",
    "        print('Of these, got only : ')\n",
    "        print(got_list & expected_list)\n",
    "        print('Got additionally : ')\n",
    "        print(got_list-expected_list)\n",
    "        print('...and we are missing : ')\n",
    "        print(expected_list-got_list)\n",
    "    \n",
    "    ######################\n",
    "    # CERTIFY INDIVIDUAL RESULTS THAT REQUIRE SPECIAL HANDLING\n",
    "    # NUMERICS\n",
    "    list_numeric_labs = ['ALT','AST','AKP','DBILI','TBILI', 'CR','INR','IGG']\n",
    "\n",
    "    lfts['above_below_assay_limit'] = 0\n",
    "\n",
    "    fil = lfts.test_desc.isin(list_numeric_labs)\n",
    "\n",
    "    # def upper and lower bound finder\n",
    "    def upper_lower_bound_finder(row):\n",
    "        if not isinstance(row.result, float):\n",
    "            if '<' in row.result:\n",
    "                return -1\n",
    "            elif '>' in row.result:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    # def upper and lower bound finder\n",
    "    def remove_gg_ll_signs(row):\n",
    "        if isinstance(row, str):\n",
    "            if '<' in row:\n",
    "                return row.replace('<','')\n",
    "            elif '>' in row:\n",
    "                return row.replace('>','')\n",
    "            else:\n",
    "                return row\n",
    "\n",
    "    # first mark whether a result is at the upper or lower limit (otherwise NaN)\n",
    "    lfts.loc[fil, 'above_below_assay_limit'] = lfts.loc[fil, ['result', 'above_below_assay_limit']].apply(upper_lower_bound_finder, axis=1)\n",
    "\n",
    "    # remove greater than and less than signs\n",
    "    lfts.loc[fil, 'result'] = lfts.loc[fil, 'result'].apply(remove_gg_ll_signs)\n",
    "    \n",
    "    # now that we've removed greater than and less than signs, data should be numeric. Anything non-numeric (in this category) is crap, \n",
    "    # e.g., \"REFUSED\", \"HEMOLYZED\" etc\n",
    "    \n",
    "    # filter for coercing data to numeric, catching NaNs (text)\n",
    "    print('Removing non-numeric result values...')\n",
    "    lfts.loc[fil, 'result'] = pd.to_numeric(lfts.loc[fil, 'result'], errors='coerce')\n",
    "    # hold on to removed rows -- error checking\n",
    "    removed_rows=lfts[(fil) & lfts.result.isnull()].copy()\n",
    "    # remove these rows\n",
    "    lfts.drop(lfts[(fil) & lfts.result.isnull()].index, inplace=True)\n",
    "    \n",
    "    \n",
    "    ######################\n",
    "    # CERTIFY INDIVIDUAL RESULTS THAT REQUIRE SPECIAL HANDLING\n",
    "    # ANTIBODY TITERS\n",
    "    \n",
    "    list_titer_labs = ['LKM','ASMA', 'ANA', 'AMA', 'SLA']\n",
    "\n",
    "    fil_ab = lfts.test_desc.isin(list_titer_labs)\n",
    "    \n",
    "    if not lfts[fil_ab].empty:\n",
    "\n",
    "        def titer_interp(row):\n",
    "            import math\n",
    "            import re\n",
    "\n",
    "            bool_switched_result_resultext=False\n",
    "\n",
    "            if isinstance(row.result, float):\n",
    "                if row.result < 20: # antibody titer cutoff for LKM and works for ANA, ASMA\n",
    "                    return 0\n",
    "                elif math.isnan(row.result):\n",
    "                    if isinstance(row.Result_Text,str):\n",
    "                        this_string=row.Result_Text\n",
    "                        bool_switched_result_resultext=True\n",
    "                    else:\n",
    "                        return row.result\n",
    "                else:\n",
    "                    return row.result\n",
    "\n",
    "            # function I'm going to use a bunch of times:\n",
    "            def all_numbers_within_n_chars_of_word(text, word, n):\n",
    "                idx_zero_pos_of_word=text.lower().find(word)\n",
    "                if len(text[idx_zero_pos_of_word:]) < n:\n",
    "                    sub_string_to_return=text[idx_zero_pos_of_word:]\n",
    "                else:\n",
    "                    sub_string_to_return=text[idx_zero_pos_of_word:idx_zero_pos_of_word+n]\n",
    "\n",
    "                return find_all_num_in_string(sub_string_to_return)\n",
    "\n",
    "            # CASES AT THIS POINT. Either this_string holds a swapped text or doesn't. Any string in result hasn't been accounted for yet\n",
    "            # most of the logic will be shared, nevertheless keep them separate because, e.g., a single number in result is usually the value we want\n",
    "            #  while a single number in result text might not be\n",
    "\n",
    "            if isinstance(row.result, str):\n",
    "                numbers = find_all_num_in_string(row.result)\n",
    "\n",
    "                if '<' in row.result:\n",
    "                    return 0\n",
    "                elif 'ANA SCREEN POSITIVE, TITRE PERFORMED' in row.result or 'POSITIVE - SEE TITER' in row.result:\n",
    "                    # this category will get removed; there's always a titer separately reported at same time point\n",
    "                    return 'see titer'\n",
    "                \n",
    "                    \n",
    "                ## FIRST IDENTIFY POSITIVELY OR NEGATIVELY IDENTIFIED EXAMPLES\n",
    "                elif 'negative' in row.result.lower() and not 'positive' in row.result.lower():\n",
    "                    return 0\n",
    "                elif 'positive' in row.result.lower() and not 'negative' in row.result.lower():\n",
    "                    \n",
    "                    if len(numbers) == 1:\n",
    "                        return numbers[0]\n",
    "                    elif len(numbers) == 2 and numbers[0]==1:\n",
    "                        return numbers[1]\n",
    "                    \n",
    "                    elif len(numbers) == 4 and numbers[2]==1:\n",
    "                        return numbers[3]\n",
    "                    \n",
    "                    else: \n",
    "                        result_text=row.Result_Text\n",
    "                        if isinstance(result_text,str):\n",
    "                            \n",
    "                            ## Some cases doenst contain the word 'positive' in Result_text which is generating a random incorrect result\n",
    "                            if 'positive' in result_text.lower():\n",
    "                                numbers=all_numbers_within_n_chars_of_word(result_text, 'positive', n=26)\n",
    "                                if len(numbers) == 1:\n",
    "                                    return numbers[0]\n",
    "                                elif len(numbers) == 2 and numbers[0]==1:\n",
    "                                    return numbers[1]\n",
    "                                elif len(numbers) > 2:\n",
    "                                    this_string=row.Result_Text.lower()\n",
    "                                    if isinstance(this_string,str):\n",
    "                                        idx_titer=this_string.find('titer')\n",
    "                                        if len(row.Result_Text[idx_titer:]) <= 17:\n",
    "                                            search_text=row.Result_Text[idx_titer:]\n",
    "                                        else:\n",
    "                                            search_text=row.Result_Text[idx_titer:idx_titer+17]\n",
    "                                        numbers=find_all_num_in_string(search_text)\n",
    "                                        if len(numbers) == 1:\n",
    "                                            return numbers[0]\n",
    "                                        elif len(numbers) == 2 and numbers[0]==1:\n",
    "                                            return numbers[1]\n",
    "                            else:\n",
    "                                return 20\n",
    "\n",
    "                elif 'positive' in row.result.lower() and 'negative' in row.result.lower():\n",
    "                    # both pos and neg present; find the text following the 'positive' word (26 chars)\n",
    "                    numbers=all_numbers_within_n_chars_of_word(row.result, 'positive', n=26)\n",
    "                    if len(numbers) == 1:\n",
    "                        return numbers[0]\n",
    "                    elif len(numbers) == 2 and numbers[0]==1:\n",
    "                        return numbers[1]\n",
    "                    else:\n",
    "                        # possible it's farther out?\n",
    "                        text=row.result[index_positive:]\n",
    "                        numbers=find_all_num_in_string(text)\n",
    "                        print('pos & neg text present but no value within 26 chars; went out on a limb and returning : ' + str(numbers[1]))\n",
    "                        return numbers[1]\n",
    "\n",
    "                # okay, so the words 'positive' and 'negative' aren't present\n",
    "                elif len(numbers) == 1:\n",
    "                    return numbers[0]\n",
    "                elif len(numbers) == 2 and numbers[0]==1:\n",
    "                    # pair of numbers returned; make sure the first value is 1\n",
    "                    return numbers[1]\n",
    "\n",
    "            # CASES AT THIS POINT. Either this_string holds a swapped text or doesn't. Any interpretable string has been returned\n",
    "\n",
    "            if bool_switched_result_resultext:\n",
    "                numbers = find_all_num_in_string(this_string)\n",
    "\n",
    "                ## FIRST IDENTIFY POSITIVELY OR NEGATIVELY IDENTIFIED EXAMPLES\n",
    "                if 'negative' in this_string.lower() and not 'positive' in this_string.lower():\n",
    "                    return 0\n",
    "                elif 'positive' in this_string.lower() and not 'negative' in this_string.lower():\n",
    "                    ## Positive at logic is not working when there are 4 numbers. Removed the if statement\n",
    "                    ## Example: Positive at 1:40 and 1:160 (endpoint)\n",
    "                    ## Finding all numbers within a close proximity of 'positive' seems to be working for all cases\n",
    "                    numbers=all_numbers_within_n_chars_of_word(this_string, 'positive', n=26)\n",
    "                    if len(numbers) == 1:\n",
    "                        return numbers[0]\n",
    "                    elif len(numbers) == 2 and numbers[0]==1:\n",
    "                        return numbers[1]\n",
    "                    elif len(numbers) == 4 and numbers[2]==1:\n",
    "                        return numbers[3]\n",
    "                    elif len(numbers)>0:\n",
    "                        max_num = max(numbers)\n",
    "                        print('positive only in result text but neither 1 nor 2 numbers, going on limb and taking : ' + str(max_num))\n",
    "                        print(this_string)\n",
    "                        return max_num\n",
    "                    \n",
    "                elif 'positive' in this_string.lower() and 'negative' in this_string.lower():\n",
    "                    # both pos and neg present; find the text following the 'positive' word (15 chars)\n",
    "                    index_positive=this_string.lower().find('positive')\n",
    "                    if len(this_string[index_positive:]) <= 26:\n",
    "                        text=this_string[index_positive:]\n",
    "                    else:\n",
    "                        text=this_string[index_positive:index_positive+26]\n",
    "                    numbers=find_all_num_in_string(text)\n",
    "        #             print(text)\n",
    "        #             print(numbers)\n",
    "                    if len(numbers) == 1:\n",
    "                        return numbers[0]\n",
    "                    elif len(numbers) == 2 and numbers[0]==1:\n",
    "                        return numbers[1]\n",
    "                    elif len(numbers) == 4:\n",
    "                        return max(numbers)\n",
    "                    elif 'negative at' in this_string.lower():\n",
    "                        return 0\n",
    "                # okay, so the words 'positive' and 'negative' aren't present\n",
    "                elif len(numbers) == 1:\n",
    "                    return numbers[0]\n",
    "                elif len(numbers) == 2 and numbers[0]==1:\n",
    "                    return numbers[1]\n",
    "\n",
    "\n",
    "            return row.result\n",
    "\n",
    "        # first mark whether a result is at the upper or lower limit (otherwise NaN)\n",
    "        lfts.loc[fil_ab, 'result'] = lfts.loc[fil_ab, ['result', 'Result_Text']].apply(titer_interp, axis=1)\n",
    "\n",
    "        print('Cleaning up antibody titer results...')\n",
    "        lfts.loc[fil_ab, 'result'] = pd.to_numeric(lfts.loc[fil_ab, 'result'], errors='coerce')\n",
    "        # hold on to removed rows -- error checking\n",
    "        removed_rows_ab=lfts[(fil_ab) & lfts.result.isnull()].copy()\n",
    "        # remove these reows\n",
    "        lfts.drop(lfts[(fil_ab) & lfts.result.isnull()].index, inplace=True)\n",
    "    \n",
    "        ######################\n",
    "        # COMBINE DISCARDED DATA\n",
    "        removed_rows = pd.concat([removed_rows,removed_rows_ab],axis=0)\n",
    "    \n",
    "    ######################\n",
    "    # REMOVE UNUSED/DISTRACTING/PRECURSOR COLUMNS\n",
    "    if clean_columns:\n",
    "        # easier to define as what we want to keep\n",
    "        lfts = lfts[['EMPI', 'test_desc', 'datetime', 'result_orig', 'result', 'MRN_Type', 'MRN', 'Test_Description', 'Result_Text', 'above_below_assay_limit']].copy()\n",
    "        \n",
    "#     from IPython import embed; embed()\n",
    "        \n",
    "    # enforce the EMPI column is strings for later\n",
    "    lfts['EMPI'] = lfts.EMPI.astype(str)\n",
    "        \n",
    "    print('...Done')\n",
    "    \n",
    "    return lfts, removed_rows\n",
    "\n",
    "def find_all_num_in_string(sentence):\n",
    "    # accepts a sentence; returns an array of all numbers in the sentence\n",
    "    import re\n",
    "    \n",
    "    s = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*', sentence)]\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def load_meds(path, delimiter='|', datetime_col='Medication_Date', prune=True):\n",
    "    import pandas as pd\n",
    "    \n",
    "    # load a medications record\n",
    "    \n",
    "    meds = pd.read_csv('data/AIH_cohort/WG08_20201109_094857_Med.txt', delimiter=delimiter, dtype=str)\n",
    "    # enforce the EMPI column is strings for later\n",
    "    \n",
    "    meds['datetime'] = pd.to_datetime(meds.loc[:,datetime_col])\n",
    "    \n",
    "    return meds[['EMPI', 'EPIC_PMRN', 'MRN_Type', 'MRN', 'datetime', 'Medication', 'Code_Type', 'Code', 'Quantity', 'Inpatient_Outpatient']]\n",
    "\n",
    "def get_table_traj(table,empi):\n",
    "    filter_pt = (table.EMPI == empi)\n",
    "    this_traj = table[filter_pt].sort_values(by='datetime').reset_index(drop=True).copy()\n",
    "    \n",
    "    return this_traj\n",
    "\n",
    "def get_traj(table, empi, lab='ALT'):\n",
    "    '''get_traj(table, empi, lab='ALT')\n",
    "    DESC: accepts the input table and a particular unique identifier EMPI and returns a table filtered for that EMPI for the lab requested\n",
    "    table: the input data table containing many patients (many EMPIs)\n",
    "    empi: the empi requested\n",
    "    lab: the lab requested, options are 'ALT','AST', 'INR'; it directly filters on this expression so works for arbitrary lab type\n",
    "    \n",
    "    WARNINGS:\n",
    "    - AST and ALT have been made uniform (e.g., SGPT=>ALT, Alt=>ALT etc), but not any other values yet; need to preprocess and homogenize tbili, inr etc\n",
    "    '''\n",
    "    # accepts a data table of lft trajectories and returns a dict containing dataframes for each separate peak\n",
    "    filter_pt = (table.EMPI == empi) & (table.test_desc == lab)\n",
    "    this_traj = table[filter_pt].sort_values(by='datetime').reset_index(drop=True).copy()\n",
    "    \n",
    "    xlab = this_traj.result.astype(float).to_numpy()\n",
    "    tseries=to_np_timeseries(this_traj)\n",
    "        \n",
    "    return xlab, tseries\n",
    "\n",
    "def get_peaks(x,t,d=[], dates_dt=28, opt_prominence=25, plot_peaks=True, min_height=None, min_datapoints=0, allow_edge_peaks=False, options='peak_to_min', strict_monotonic=True):\n",
    "    '''get_peaks\n",
    "    \n",
    "    Parameters:\n",
    "    x-ALT or AST data, np array\n",
    "    t-matched t trajectory for alt/ast data\n",
    "    d-(optional) date list in epoch UTC time, same as t. If d is not empty, only return peaks within dates_dt of passed dates'''\n",
    "    \n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.signal import find_peaks\n",
    "    \n",
    "    ####################\n",
    "    # First, identify the peaks\n",
    "    ####################\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        print('get_peaks(): The x vector passed is empty, no peaks to assign')\n",
    "        # empty - don't return peaks\n",
    "        return None, 0, None, None\n",
    "    \n",
    "    # get the peaks\n",
    "    peaks, trash = find_peaks(x, height=[min_height, None], prominence=[opt_prominence,None])\n",
    "    \n",
    "    # are we allowed to call the very first or very last value a peak? if so, algo is: add the lowest value to beginning and end\n",
    "    #  if that makes a peak at beginning or end, update the peaks indices. drop the bases found this way (not relevant)\n",
    "    if allow_edge_peaks:\n",
    "        # add lowest value to beginning and end of signal\n",
    "        x_temp = np.concatenate(([min(x)],x,[min(x)]))\n",
    "        # stores peak indices but not bases; ie don't overwrite peak_dict from above\n",
    "        peaks, trash = find_peaks(x_temp, height=[min_height, None], prominence=[opt_prominence,None])\n",
    "        # peak location offset by one since we added one to beginning; subtract one to fix\n",
    "        peaks=peaks-1\n",
    "    \n",
    "    n_peaks=len(peaks)\n",
    "    \n",
    "    # make sure there are peaks; if not, return None\n",
    "    if n_peaks == 0:\n",
    "        return None, 0, None, None\n",
    "    \n",
    "#     from IPython import embed; embed()\n",
    "    \n",
    "    # OKAY, there are peaks. If we want specific dates, apply that filter now\n",
    "    # pseudocode. For each peak, check whether within dates_dt for each date\n",
    "    if d:\n",
    "        peaks_dates=np.array([])\n",
    "        # if d just ensures d is not empty, if d empty move on with all peaks..\n",
    "        for k in range(0, n_peaks):\n",
    "            current_peak=peaks[k]\n",
    "            current_t=t[current_peak]\n",
    "            has_date_match=False\n",
    "            \n",
    "            for date_UTC in d:\n",
    "                if date_UTC-dates_dt <= current_t and date_UTC+dates_dt >= current_t:\n",
    "                    has_date_match=True\n",
    "            if has_date_match:\n",
    "                peaks_dates=np.append(peaks_dates, current_peak)\n",
    "        \n",
    "        # we could be back to zero peaks\n",
    "        peaks=peaks_dates.astype(int)\n",
    "        \n",
    "        n_peaks=len(peaks)\n",
    "        \n",
    "        # make sure there are peaks; if not, return None\n",
    "        if n_peaks == 0:\n",
    "            print('There were peaks, but none within dt_dates')\n",
    "            return None, 0, None, None\n",
    "    \n",
    "    ####################\n",
    "    # Now identify the intervals.\n",
    "    ## 'peak_to_min' behavior is LL=peak, UL=min\n",
    "    ####################\n",
    "    \n",
    "    peaks_final=[]\n",
    "    LL_vec=[]\n",
    "    UL_vec=[]\n",
    "              \n",
    "    if options=='peak_to_min' or options=='peak_to_peak':\n",
    "        for j in range(0,n_peaks):\n",
    "            # is there another peak or is this the last one?\n",
    "            if j==n_peaks-1:\n",
    "                # this is last one; UL is index of last element of x\n",
    "                UL=len(x)\n",
    "            else:\n",
    "                # use the next peak as the initial end\n",
    "                UL=peaks[j+1]\n",
    "            # first pass: all values between this peak and the next:\n",
    "            LL = peaks[j]\n",
    "            # subset xalt to this region between peaks\n",
    "            xalt_sub=x[LL:UL]\n",
    "            \n",
    "            if options=='peak_to_min':\n",
    "                # find min on this region (second half); \n",
    "                min_idx_sub = np.argmin(xalt_sub)\n",
    "                # to get the global index w.r.t xalt (rather than xalt_sub) add the LL\n",
    "                min_idx = LL + min_idx_sub\n",
    "                # truncate again to this min value (inclusive, thus the +1); update UL\n",
    "                UL=min_idx+1\n",
    "                \n",
    "            if strict_monotonic:\n",
    "                # peak finder finds all of : peak > min_height AND prominence > input_prominence\n",
    "                # - unfortunately this means a peak of 26 (less than min_height) over a baseline of 12 won't get called\n",
    "                # this code enforces a strict monotonic decline\n",
    "                continue_loop=True\n",
    "                n=1\n",
    "                new_x = np.array([x[LL]])\n",
    "                while continue_loop==True and n+LL<UL:\n",
    "                    if x[LL+n] < x[LL+n-1]:\n",
    "                        new_x = np.append(new_x,x[LL+n])\n",
    "                        \n",
    "                    else:\n",
    "                        continue_loop=False\n",
    "                    n=n+1\n",
    "                UL=LL+n-1\n",
    "            \n",
    "            # accumulate if sufficient number of datapoints\n",
    "            num_datapoints=len(x[LL:UL])\n",
    "            if num_datapoints >= min_datapoints:\n",
    "                peaks_final.append(peaks[j])\n",
    "                LL_vec.append(LL)\n",
    "                UL_vec.append(UL)\n",
    "    \n",
    "    LL_vec=np.array(LL_vec)\n",
    "    UL_vec=np.array(UL_vec)\n",
    "    peaks_vec=np.array(peaks_final)\n",
    "    \n",
    "    if plot_peaks:\n",
    "        plt.plot(t,x)\n",
    "        plt.plot(t[peaks_final], x[peaks_final],'x')\n",
    "        for i in range(0,len(LL_vec)):\n",
    "            t_LL=t[LL_vec[i]]\n",
    "            t_UL=t[UL_vec[i]-1]\n",
    "            plt.axvspan(t_LL, t_UL, facecolor='grey', alpha=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    return peaks_final, len(peaks_final), LL_vec, UL_vec\n",
    "\n",
    "\n",
    "\n",
    "def plot_lfttraj(tseries,x,peaks=None, bases=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.plot(tseries,x)\n",
    "    \n",
    "    if not peaks == None:\n",
    "        plt.plot(tseries[peaks], x[peaks],'x')\n",
    "    if not bases == None:\n",
    "        plt.plot(tseries[peaks], x[bases],'ro')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "## THESE DATETIME FUNCTIONS ARE ALL OVER THE PLACE. \n",
    "# - not timezone aware\n",
    "# - should probably use built in pandas functions, don't need to calculate it myself..\n",
    "# - epoch_days_to_date is somewhat modernized, still no timezone awareness\n",
    "\n",
    "def to_np_timeseries(df, dtype='df', datetime_col='datetime'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    if dtype=='df':\n",
    "        #native behavior for backward compatibility; assumes the column is datetime\n",
    "        epoch_time=pd.Timestamp('1970-01-01 00:00:00')\n",
    "        t_dseries = df.apply(lambda x: (x[datetime_col]-epoch_time).total_seconds(),axis=1)\n",
    "        t_days = t_dseries.to_numpy()/(3600*24)\n",
    "        \n",
    "    elif dtype=='ds':\n",
    "        epoch_time=pd.Timestamp('1970-01-01 00:00:00')\n",
    "        t_dseries = df.apply(lambda x: (x-epoch_time).total_seconds())\n",
    "        t_days = t_dseries.to_numpy()/(3600*24)\n",
    "    \n",
    "    return t_days\n",
    "\n",
    "def epoch_days_to_date(epoch_day):\n",
    "    ''' converts UTC (time since epoch) in units of days since epoch to a datetime timestamp\n",
    "    if input is scalar, returns scalar\n",
    "    if input is np.array, returns datetime array\n",
    "    warnings:\n",
    "    - not timezone aware\n",
    "    '''\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    \n",
    "    if np.isscalar(epoch_day):\n",
    "        epoch_seconds=int(epoch_day*(3600*24))\n",
    "        out = datetime.utcfromtimestamp(epoch_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        epoch_seconds = np.rint(epoch_day*(3600*24))\n",
    "        out = pd.to_datetime(epoch_seconds, unit='s')\n",
    "    \n",
    "    return out\n",
    "\n",
    "def epoch_date_to_days(pd_datetime):\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    epoch_time=pd.Timestamp('1970-01-01 00:00:00')\n",
    "    epoch_seconds = (pd_datetime-epoch_time).total_seconds()\n",
    "    \n",
    "    epoch_days=epoch_seconds/(3600*24)\n",
    "    \n",
    "    return epoch_days\n",
    "\n",
    "def rect_trapz(t,x,delay=3):\n",
    "    '''integration with rectangular integration when t(i)-t(i-1) > delay '''\n",
    "    import scipy\n",
    "    \n",
    "    if len(t)==1:\n",
    "        # only one element\n",
    "        return 0\n",
    "        \n",
    "    cum_aoc = 0\n",
    "    for i in range(0,len(x)-1):\n",
    "        tau = t[i+1]-t[i]\n",
    "        if tau > delay:\n",
    "            # rectangular integration\n",
    "            cum_aoc = cum_aoc+(tau*x[i])\n",
    "        else:\n",
    "            # trap integration\n",
    "            cum_aoc = cum_aoc + scipy.trapz(x[i:i+1],x=t[i:i+1])\n",
    "            \n",
    "    return cum_aoc\n",
    "            \n",
    "def running_mean(x, N):\n",
    "    import numpy as np\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "\n",
    "def discrete_autocorr(x,h=1):\n",
    "    # reference:\n",
    "    # 1.Kuzmič, P., Lorenz, T. & Reinstein, J. Analysis of residuals from enzyme kinetic and protein folding experiments \n",
    "    #  in the presence of correlated experimental noise. Analytical Biochemistry 395, 1–7 (2009).\n",
    "    import numpy as np\n",
    "    \n",
    "    # mean\n",
    "    xbar=x.mean()\n",
    "    n_d=len(x)\n",
    "    \n",
    "    xh=x[h:]\n",
    "    C_h=(1/n_d)*np.sum((x[:-1]-xbar)*(xh-xbar))\n",
    "    C_0=(1/n_d)*np.sum((x-xbar)**2)    \n",
    "    \n",
    "    R_h=C_h/C_0\n",
    "    \n",
    "    z1_alpha2=None\n",
    "    \n",
    "    if h==1:\n",
    "        # calculate whether the residual autocorrelation could occur by random chance given the null hypothesis z=0\n",
    "        z1_alpha2=R_h*np.sqrt(n_d)\n",
    "    \n",
    "    return R_h, z1_alpha2\n",
    "\n",
    "def qualitative_error(t, x, residuals):\n",
    "    # I am making this up\n",
    "    import numpy as np\n",
    "    \n",
    "    model_vals = x+residuals\n",
    "       \n",
    "    window_range_min = np.min(np.append(model_vals,x))\n",
    "    window_range_max = np.max(np.append(model_vals,x))\n",
    "    window_range = window_range_max-window_range_min\n",
    "    \n",
    "    qual_err = (1/len(x))*np.sum(np.absolute(residuals)/(window_range))\n",
    "    \n",
    "    return qual_err\n",
    "\n",
    "def MAPE(t, x, residuals):\n",
    "    import numpy as np\n",
    "    \n",
    "    MAPE = (1/len(x))*np.sum(np.absolute(residuals)/(x))\n",
    "    \n",
    "    return MAPE\n",
    "\n",
    "def nRMSD(t, x, residuals):\n",
    "    import numpy as np\n",
    "    \n",
    "    y_min = np.min(x)\n",
    "    y_max = np.max(x)\n",
    "    window_range = y_max-y_min\n",
    "    \n",
    "    nRMSD = (1/window_range)*np.sqrt(np.sum(residuals**2/len(x)))\n",
    "    \n",
    "    return nRMSD\n",
    "\n",
    "def runs_of_signs(residuals):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    n_d=len(residuals)\n",
    "    # convert to -1, 0, or 1\n",
    "    signs=np.sign(residuals)\n",
    "    # remove zeroes (they don't count)\n",
    "    signs = signs[signs != 0]\n",
    "    # compute the sum of the offest signs (ie, element 0 + element 1, 1+2. )\n",
    "    signs_d = signs[0:-1]+signs[1:]\n",
    "    # if there is a crossover, or change of signs, -1 + 1 = 0. Count these events. Add one (first run always counts, but no 'crossover' for first event)\n",
    "    n_runs = len(signs_d[signs_d==0])+1\n",
    "    n_pos = len(signs[signs==1])\n",
    "    \n",
    "    mu = 2*n_pos*(n_d-n_pos)/n_d + 1\n",
    "    sig_squared = (mu-1)*(mu-2)/(n_d-1)\n",
    "    \n",
    "    z = (n_runs-mu+.5)/np.sqrt(sig_squared)\n",
    "    \n",
    "    return z\n",
    "\n",
    "def on_interval(t1,t2,LL2,UL2,dt_LL=4,dt_UL=5):\n",
    "    # accepts an already truncated interval of interest (t1) and asks for a matching interval among LL2[] UL2[] on t2\n",
    "    # returns if there is an interval that starts and ends on a similar time scale\n",
    "    # LL and UL are paired vectors for ranges\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    t1_LL=t1[0]\n",
    "    t1_UL=t1[-1]\n",
    "    \n",
    "    t2_LL=t2[LL2]\n",
    "    t2_UL=t2[UL2-1]\n",
    "    \n",
    "    out_LL, out_idx_trash = find_nearest(t2_LL,t1_LL)\n",
    "    idx_LL_closest = np.where(LL2==np.where(t2==out_LL)[0][0])[0][0]\n",
    "    \n",
    "    out_UL = t2[UL2[idx_LL_closest]-1]\n",
    "    best_idx=-1\n",
    "    \n",
    "    # is the closest LL pretty close?\n",
    "    if out_LL < t1_LL+dt_LL and out_LL > t1_LL-dt_LL:\n",
    "        #yes -- within dt for LL. \n",
    "        best_idx = idx_LL_closest\n",
    "        # for now, don't dictate the UL has to be similar. In theory if the LL is close that's all that matters...\n",
    "        \n",
    "#         if out_UL < t1_UL+dt_UL and out_UL > t1_UL-dt_UL:\n",
    "#             best_idx = idx_LL_closest\n",
    "        \n",
    "    return best_idx\n",
    "    \n",
    "def find_nearest(array, value):\n",
    "    import numpy as np\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx], idx\n",
    "\n",
    "def UTC_days_to_date(UTC_in_days):\n",
    "    from datetime import datetime\n",
    "    date_vector=[]\n",
    "    try: \n",
    "        for utc_day in UTC_in_days:\n",
    "            ts = int(utc_day*24*3600)\n",
    "    #         date_vector.append(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d'))\n",
    "            date_vector.append(datetime.utcfromtimestamp(ts))\n",
    "    except:\n",
    "        ts = int(UTC_in_days*24*3600)\n",
    "        date_vector.append(datetime.utcfromtimestamp(ts))\n",
    "\n",
    "    return date_vector\n",
    "\n",
    "def write_results(directory,filename_prefix, df, loopfit_stats):\n",
    "    import pandas as pd\n",
    "    \n",
    "    fname=directory+filename_prefix\n",
    "    df.to_csv(fname+'_param_fits.csv')\n",
    "    with open(fname+'_loopfit_stats.txt', 'w') as f:\n",
    "        print(loopfit_stats, file=f)\n",
    "        \n",
    "    return None\n",
    "        \n",
    "def get_nearest_lab(lfts,ids,t0,lab='INR'):\n",
    "    # convention is nearest time minus t0 (passed value), so negative = before t0, positive=after t0\n",
    "    \n",
    "    x, t = get_traj(lfts, ids, lab=lab)\n",
    "    if len(x) == 0:\n",
    "        # this lab was never obtained\n",
    "        return None, None, None\n",
    "    else:\n",
    "        nearest_t, nearest_t_idx = find_nearest(t, value=t0)\n",
    "        nearest_x=x[nearest_t_idx]\n",
    "\n",
    "        lab_value=nearest_x\n",
    "        lab_time=nearest_t\n",
    "        lab_dt=nearest_t-t0\n",
    "    \n",
    "    return lab_value, lab_time, lab_dt\n",
    "\n",
    "def append_nearest_lab(df_in, lfts, suffix, labs_meta_list=['ALT', 'AST', 'AKP', 'INR', 'TBILI', 'DBILI', 'CR', 'IGG']):\n",
    "    ''' append_nearest_lab\n",
    "    \n",
    "    DESC: for AST/ALT fit dataframe, goto the t0_+suffix column to get the time of the fit-peak, then\n",
    "      using lfts as a lookup database, find the nearest labs_meta_list labs and add them along with their time/dt as cols'''\n",
    "    import pandas as pd\n",
    "    \n",
    "    # remove the 'calling' lab (suffix) from meta list\n",
    "    if suffix in labs_meta_list:\n",
    "        labs_meta_list.remove(suffix)\n",
    "    \n",
    "    df = df_in.copy()\n",
    "    \n",
    "    # merge-asof will lose indice values, store them here\n",
    "    df['idx_peak_'+suffix] = df.index\n",
    "    \n",
    "    # merge_asof requires sorted values; sort on epoch_days for the input frame\n",
    "    df.sort_values(by='t0_'+suffix, ascending=True, inplace=True)\n",
    "    \n",
    "    # these can all become inputs / parameters eventually..\n",
    "    exactly_match1='id_'+suffix\n",
    "    exactly_match2='EMPI'\n",
    "    time1='t0_'+suffix\n",
    "    \n",
    "    for lab in labs_meta_list:\n",
    "        # shrink the frame to only this lab\n",
    "        print('Filtering for ' + lab + '...')\n",
    "        this_lab = lfts[lfts.test_desc == lab].copy()\n",
    "        if not this_lab.empty:\n",
    "            this_lab = this_lab.rename(columns={'result': lab+'_value'})\n",
    "            # make a new column for epoch time (lfts/labs start as datetime)\n",
    "            time2=lab+'_epoch'\n",
    "            this_lab[time2] = to_np_timeseries(this_lab.datetime, dtype='ds')\n",
    "            this_lab=this_lab[['EMPI',time2,lab+'_value']].copy()\n",
    "            # sort here as well for merge_asof\n",
    "            this_lab.sort_values(by=time2,ascending=True, inplace=True)\n",
    "\n",
    "            # the magic. merge_asof merges based on nearest values rather than exact values. left_on and right_on are the columns (time) to match nearest\n",
    "            #  It will further only match nearest WITHIN left_by and right_by which must be exact, and here are the EMPI/ids\n",
    "            print('Merging nearest ' + lab + '...')\n",
    "            df = pd.merge_asof(df, this_lab, left_on=time1, right_on=time2, left_by=exactly_match1, right_by=exactly_match2, direction='nearest')\n",
    "            df.drop(columns=['EMPI'], inplace=True)\n",
    "            df[lab+'_dt'] = df[time2]-df[time1]\n",
    "        else:\n",
    "            print('No such lab in reference: ' + lab)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def append_nearest_peak(df1,suffix1,df2,suffix2,dt):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    idcolname1='id_'+suffix1\n",
    "    idcolname2='id_'+suffix2\n",
    "    t0colname1='t0_'+suffix1\n",
    "    t0colname2='t0_'+suffix2    \n",
    "    \n",
    "    df1_idx=df1.index.tolist()\n",
    "    \n",
    "    idx_to_keep1 = []\n",
    "    idx_to_keep2 = []\n",
    "    counter_keep = []\n",
    "    dt_keep = []\n",
    "    ct=0\n",
    "    \n",
    "    for idx in df1_idx:\n",
    "        id2match=df1.loc[idx,idcolname1]\n",
    "        t0match=df1.loc[idx,t0colname1]\n",
    "        # screen df2 for the same value\n",
    "        df2_sub=df2[df2.loc[:,idcolname2]==id2match]\n",
    "        \n",
    "        if not df2_sub.empty:\n",
    "            # convert t0 in df2 to a numparray to ease the difficulty of finding the value\n",
    "            t0_2=df2_sub.loc[:,t0colname2].to_numpy()\n",
    "\n",
    "            # get the closest value\n",
    "            val_nearest, idx_nearest = find_nearest(t0_2,t0match)\n",
    "            \n",
    "            # calculate difference; reject if too far apart\n",
    "            diff=val_nearest-t0match\n",
    "            \n",
    "            if np.absolute(diff) < dt:\n",
    "                # store both indices\n",
    "                idx_to_keep1.append(idx)\n",
    "                idx_to_keep2.append(df2_sub.index[idx_nearest])\n",
    "                counter_keep.append(ct)\n",
    "                dt_keep.append(diff) # by convention, df2_t0-df1_t0\n",
    "                ct=ct+1\n",
    "                \n",
    "    # done cycling; now merge the databases and add the dt col (INNER MERGE)\n",
    "    ## this is weird because have to have the same ID to join.. so we made a new id in counter_keep. BUT. this isn't on the dataframes yet.\n",
    "    # create a ref df for each\n",
    "    match1=pd.DataFrame({'id1': counter_keep}, index=idx_to_keep1)\n",
    "    match2=pd.DataFrame({'id2': counter_keep, 't0_dt': dt_keep}, index=idx_to_keep2)\n",
    "    df1n=df1.merge(match1,left_index=True,right_index=True,how='inner')\n",
    "    df2n=df2.merge(match2,left_index=True,right_index=True,how='inner')\n",
    "    \n",
    "    # now complete the merge\n",
    "    merge_df=df1n.merge(df2n, left_on='id1',right_on='id2', how='inner')\n",
    "    \n",
    "    return merge_df, match1, match2\n",
    "\n",
    "def plot_fit(ids, plots_save, df, suffix, pause_for_input=True):\n",
    "    '''\n",
    "    ids - peak indices (not patient)'''\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    inp='y'\n",
    "    \n",
    "    if not isinstance(ids, list):\n",
    "        ids=[ids]\n",
    "        \n",
    "    for idx in ids:\n",
    "        \n",
    "        if inp =='n':\n",
    "            break\n",
    "        \n",
    "        t=plots_save[idx]['t']\n",
    "        x=plots_save[idx]['x']\n",
    "        xfit=plots_save[idx]['xfit']\n",
    "        t_highres=plots_save[idx]['t_highres']\n",
    "        x_highres=plots_save[idx]['x_highres']\n",
    "        \n",
    "        C_track2_present=False\n",
    "        C_track3_present=False\n",
    "        try:\n",
    "            C_track2 = plots_save[idx]['C_track2']\n",
    "            d_assume = plots_save[idx]['d']\n",
    "            C_track2_present=True\n",
    "        except:\n",
    "            print('no c-track2')\n",
    "        try:\n",
    "            C_track3 = plots_save[idx]['C_track3']\n",
    "            C_track3_present=True\n",
    "        except:\n",
    "            print('no c-track3')\n",
    "        \n",
    "        # THIS IS GOING TO ERROR LATER WHEN '_' IS ADDED BACK\n",
    "        d=df.loc[idx,'d'+suffix]\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "        ax.plot(t, xfit, 'rx', label='best fit')\n",
    "        ax.plot(t, x, 'b.', label='data')\n",
    "        ax.plot(t_highres,x_highres,'--',color='tab:gray')\n",
    "        \n",
    "        if C_track3_present:\n",
    "            # plot the 3point running C values; print the d values \n",
    "            ax.plot(t[1:-1],C_track3,'ko')\n",
    "        if C_track2_present:\n",
    "            # plot the 3point running C values; print the d values \n",
    "            ax.plot(t[1:],C_track2/d_assume,'bo')\n",
    "        \n",
    "        ax.set_title(idx)\n",
    "        d_text='d={:0.3f}'.format(d)\n",
    "        ax.text(0.79, 0.95, d_text, transform=ax.transAxes, fontsize=10, verticalalignment='top', horizontalalignment='left')\n",
    "        ax.set_ylim([0,xfit[0]*1.2])\n",
    "        ax.set_xlabel('days')\n",
    "        ax.set_ylabel(suffix+' U/L')\n",
    "        fig.show()\n",
    "    \n",
    "        if pause_for_input:\n",
    "            inp=input('continue?')\n",
    "            if inp=='n':\n",
    "                return fig, ax\n",
    "                \n",
    "    return None\n",
    "\n",
    "def binned_stats(x_data,y_data,n_bins=10, statistic='mean'):\n",
    "    ''' binned_stats\n",
    "    DESC: takes in x_data, y_data, computes the histogram with n_bins, and takes the average for each bin\n",
    "    returns bin_averages, bin_centers'''\n",
    "    import numpy as np\n",
    "    from scipy.stats import binned_statistic\n",
    "    \n",
    "    if statistic=='sem':\n",
    "        bin_stat, bin_edges, binnumber = binned_statistic(x_data, y_data, statistic='std', bins=n_bins)\n",
    "        bin_counts, _trash, _trash2 = binned_statistic(x_data, y_data, statistic='count', bins=n_bins)\n",
    "        bin_stat=bin_stat/np.sqrt(bin_counts)\n",
    "        bin_centers = bin_edges[0:-1]+(bin_edges[1:]-bin_edges[0:-1])/2\n",
    "    else:\n",
    "        bin_stat, bin_edges, binnumber = binned_statistic(x_data, y_data, statistic=statistic, bins=n_bins)\n",
    "        bin_centers = bin_edges[0:-1]+(bin_edges[1:]-bin_edges[0:-1])/2\n",
    "    \n",
    "    return bin_stat, bin_centers\n",
    "\n",
    "def find_event_within(df_input1, df_input2, datetime_col1='datetime', datetime_col2='datetime', datetime_type='timestamp', by1='MRN', by2='MRN', dt=[7,14], suffix=('_path', '_meds'), asof_direction='nearest', remove=False, save_index=False):\n",
    "    ''' find_event_within(df1, df2, datetime_col1='datetime', datetime_col2='datetime', dt=[7,14])\n",
    "        DESC: takes any event list (by row) in df1, and finds events with matching by1/by2 elements (e.g., MRN) WITHIN -dt[0]\n",
    "         to dt[1] days of each row event\n",
    "         \n",
    "         df1 - the dataframe to go through row by row\n",
    "         df2 - the dataframe to search for each row of df1\n",
    "         datetime_col1/2 - the column with datetime elements we want to match within for df1 and df2 respectively\n",
    "         datetime_type - format of the datetime column. default='timestamp', alternatively='days_since_epoch'\n",
    "         by1/by2 - the EXACT match column to ensure before doing the above; MRN was the imagined use-case\n",
    "         dt - a list with two elements, default = [7, 14], which means allow a match in datetime for up to 7 days before or 14 days after\n",
    "          * TAKE THE CLOSEST ELEMENT IN THAT RANGE\n",
    "         suffix - what we want to append to duplicated columns as (), default=('_path', '_meds')\n",
    "         save_index - merge_asof will lose the index at time of merge; if true, save these to a column name\n",
    "        \n",
    "        NOTES/Warnings:\n",
    "        - Always takes the NEAREST event in df2 to a row in df1; thus, possible for events in df2 to be duplicated in multiple matches to df1 rows\n",
    "        - Use case: \n",
    "        -- take liver biopsies (df1) and a df2 of medications *already filtered for prednisone and equivalents*\n",
    "        -- return a joined dataframe of df2 elements matched to df1 elements first by MRN (by1/by2) then by range in datetime columns set by dt\n",
    "        - default behavior is to dump NA columns (ie, rows of df1 with no match in df2)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    \n",
    "    # copy the dataframes so we're not altering the original\n",
    "    df1=df_input1.copy()\n",
    "    df2=df_input2.copy()\n",
    "    \n",
    "    # copy the indices (these will be lost in merge_asof)\n",
    "    if save_index:\n",
    "        df1['orig_index'+suffix[0]] = df1.index\n",
    "        df2['orig_peak'+suffix[1]] = df2.index\n",
    "    \n",
    "    # get num cols initial\n",
    "    num_cols_df1_init=len(df1.columns.tolist())\n",
    "    \n",
    "    # sort values (required by merge_asof)\n",
    "    df1.sort_values(by=datetime_col1, ascending=True, inplace=True)\n",
    "    df2.sort_values(by=datetime_col2, ascending=True, inplace=True)\n",
    "    \n",
    "    if datetime_type == 'days_since_epoch':\n",
    "        df1['utc'+suffix[0]] = df1[datetime_col1]\n",
    "        df2['utc'+suffix[1]] = df2[datetime_col2]\n",
    "    elif datetime_type == 'timestamp':\n",
    "        # add utc column\n",
    "        df1['utc'+suffix[0]] = to_np_timeseries(df1, datetime_col=datetime_col1)\n",
    "        df2['utc'+suffix[1]] = to_np_timeseries(df2, datetime_col=datetime_col2)\n",
    "\n",
    "    # total number of days in dt range (first element = before event, second element = after event)\n",
    "    len_dt=dt[0]+dt[1]\n",
    "\n",
    "    # time offset since timedelta tolerance for merge_asof only takes balanced +/- tolerances, but want to be able to say 7 days before up to 21 days after (for example)\n",
    "    # half the total range of days\n",
    "    \n",
    "    # make a temp column capturing the date of the midway point\n",
    "    if datetime_type=='timestamp':\n",
    "        timedelta_split = pd.Timedelta(days=len_dt/2)\n",
    "        df1['dt_temp'] = df1[datetime_col1]-pd.Timedelta(days=dt[0])+timedelta_split\n",
    "    elif datetime_type=='days_since_epoch':\n",
    "        timedelta_split = len_dt/2\n",
    "        df1['dt_temp'] = df1[datetime_col1]-dt[0]+timedelta_split\n",
    "    \n",
    "    # get num cols\n",
    "    num_cols_df1=len(df1.columns.tolist())\n",
    "    \n",
    "    # the merge. \n",
    "    out_df2 = pd.merge_asof(df1, df2, left_on='dt_temp', right_on=datetime_col2, left_by=by1, right_by=by2, suffixes=suffix, direction=asof_direction, tolerance=timedelta_split)\n",
    "\n",
    "    # make sure we've found at least one match (if so, we will have added columns compared to df1)\n",
    "    if len(out_df2.columns.tolist()) > num_cols_df1:\n",
    "        print('removing na cols')\n",
    "        \n",
    "        if remove==False:\n",
    "            # get rid of NA columns by using the fact that any new column should be after len(df1 columns), and default behavior of merge_asof is NA if no match.\n",
    "            out_df3=out_df2[~out_df2.iloc[:,num_cols_df1].isna()].copy()\n",
    "            removed = out_df2[out_df2.iloc[:,num_cols_df1].isna()].copy()\n",
    "        else:\n",
    "            # remove=True, meaning invert this whole process. Keep the NAs only. Ie, if there is a drug script in the time range, REMOVE those entries. \n",
    "            out_df3=out_df2[out_df2.iloc[:,num_cols_df1].isna()].copy()\n",
    "            removed = out_df2[~out_df2.iloc[:,num_cols_df1].isna()].copy()\n",
    "        \n",
    "        # delta col name\n",
    "        timediff_name='dt' + suffix[1] + suffix[0]\n",
    "        out_df3[timediff_name] = out_df3.loc[:,'utc'+suffix[1]] -out_df3.loc[:,'utc'+suffix[0]]\n",
    "\n",
    "    out_df3.drop(labels=['dt_temp','utc'+suffix[1], 'utc'+suffix[0]], axis=1, inplace=True)\n",
    "    \n",
    "    if remove:\n",
    "        out_df3=out_df3.iloc[:, 0:num_cols_df1_init].copy()\n",
    "    \n",
    "    return out_df3, removed\n",
    "  \n",
    "    \n",
    "def save_obj(save_path, obj):\n",
    "    import pickle\n",
    "    with open(save_path + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(load_path):\n",
    "    import pickle\n",
    "    with open(load_path + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
